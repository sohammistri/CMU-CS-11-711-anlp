{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule based classifer\n",
    "\n",
    "We are building a rule based classifier for sentiment analysis of movie reviews. We have positive (`1` label), neutral (`0` label) or negative (`-1` label) sentiments.\n",
    "\n",
    "The idea is to use a list of words and identify if they are more inclined towards the positive or negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"data/sst-sentiment-text-threeclass\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ||| The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\n",
      "1 ||| The gorgeously elaborate continuation of `` The Lord of the Rings '' trilogy is so huge that a column of words can not adequately describe co-writer\\/director Peter Jackson 's expanded vision of J.R.R. Tolkien 's Middle-earth .\n",
      "1 ||| Singer\\/composer Bryan Adams contributes a slew of songs -- a few potential hits , a few more simply intrusive to the story -- but the whole package certainly captures the intended , er , spirit of the piece .\n",
      "0 ||| You 'd think by now America would have had enough of plucky British eccentrics with hearts of gold .\n",
      "1 ||| Yet the act is still charming here .\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(data_folder, \"train.txt\"), \"r\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "    for line in lines[:5]:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = [], []\n",
    "x_dev, y_dev = [], []\n",
    "x_test, y_test = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in [\"train.txt\", \"dev.txt\", \"test.txt\"]:\n",
    "    with open(os.path.join(data_folder, file_name), \"r\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "        for line in lines:\n",
    "            label, review = line.split(\"|||\")\n",
    "            label = int(label.strip())\n",
    "            \n",
    "            if \"train\" in file_name:\n",
    "                x_train.append(review)\n",
    "                y_train.append(label)\n",
    "            elif \"dev\" in file_name:\n",
    "                x_dev.append(review)\n",
    "                y_dev.append(label)\n",
    "            elif \"test\" in file_name:\n",
    "                x_test.append(review)\n",
    "                y_test.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(x_train) == len(y_train)\n",
    "assert len(x_dev) == len(y_dev)\n",
    "assert len(x_test) == len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8544, 1101, 2210)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train), len(x_dev), len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write code to extract features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List from prof code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_words_prof = ['love', 'good', 'nice', 'great', 'enjoy', 'enjoyed']\n",
    "bad_words_prof = ['hate', 'bad', 'terrible', 'disappointing', 'sad', 'lost', 'angry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(x, good_words, bad_words):\n",
    "    features = {\"good_words\": 0, \"bad_words\": 0}\n",
    "    words = x.split(' ')\n",
    "\n",
    "    for word in words:\n",
    "        word = word.lower().strip()\n",
    "        if word in good_words:\n",
    "            features[\"good_words\"] += 1.0\n",
    "        if word in bad_words:\n",
    "            features[\"bad_words\"] += 1.0\n",
    "\n",
    "    features[\"bias\"] = 1.0\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a list of initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_weights = {\"good_words\": 1.0, \"bad_words\": -1.0, \"bias\": 0.5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract score and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(x, good_words, bad_words):\n",
    "    features = extract_features(x, good_words, bad_words)\n",
    "\n",
    "    score = 0.0\n",
    "    for key in features.keys():\n",
    "        score += feature_weights[key] * features[key]\n",
    "\n",
    "    if score < 0:\n",
    "        return -1\n",
    "    elif score > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(x_arr, y_arr, good_words, bad_words):\n",
    "    y_pred_arr = []\n",
    "    for x in x_arr:\n",
    "        y_pred_arr.append(get_label(x, good_words, bad_words))\n",
    "\n",
    "    correct_predictions = sum(p == t for p, t in zip(y_pred_arr, y_arr))\n",
    "    accuracy = (correct_predictions / len(y_arr)) * 100.00\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now get accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 43.56%\n",
      "Dev Accuracy: 42.23%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = get_accuracy(x_train, y_train, good_words_prof, bad_words_prof)\n",
    "dev_accuracy = get_accuracy(x_dev, y_dev, good_words_prof, bad_words_prof)\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy:.2f}%\")\n",
    "print(f\"Dev Accuracy: {dev_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
