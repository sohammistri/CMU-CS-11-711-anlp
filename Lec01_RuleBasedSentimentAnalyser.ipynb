{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sohammistri/CMU-CS-11-711-anlp/blob/main/Lec01_RuleBasedSentimentAnalyser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAHIxG3232So"
      },
      "source": [
        "# Rule based sentiment analysis\n",
        "\n",
        "This is my playground for the sentiment analysis code given by Prof. Graham Neubig. The original code can be accessed here [link](https://github.com/neubig/anlp-code/tree/main/01-rulebasedclassifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaZJqT__4Q18"
      },
      "source": [
        "## Import data\n",
        "\n",
        "Datasets are available and described here [link](https://github.com/neubig/anlp-code/tree/main/data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xepU4C8L4QWi",
        "outputId": "78415362-95af-4d71-f152-abe16d8a5bec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'anlp-code'...\n",
            "remote: Enumerating objects: 52, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 52 (delta 15), reused 47 (delta 11), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (52/52), 2.45 MiB | 6.81 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/neubig/anlp-code.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "L4dsu5rY3wLq"
      },
      "outputs": [],
      "source": [
        "DATA_ROOT = \"/content/anlp-code/data/sst-sentiment-text-threeclass\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrF06erG5ExZ"
      },
      "source": [
        "## Replicating the approach followed in class.\n",
        "\n",
        "Here I will be replicating the positive and negative word frequency based approach followed in class. With few modifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wXlkFveW5ESc"
      },
      "outputs": [],
      "source": [
        "def get_score(sent, pos_words, neg_words, weights, preprocess=None):\n",
        "  \"\"\"\n",
        "  @param sent: The input sentence\n",
        "  @param pos_words: List of positive words\n",
        "  @param neg_words: List of negative words\n",
        "  @param weights: Set of weights given to [pos,neg,bias]\n",
        "  @preprocess: Preprocess the sent if needed, by default None\n",
        "  @return: +1 if positive, -1 if negative, 0 if neutral\n",
        "  \"\"\"\n",
        "\n",
        "  if preprocess is not None:\n",
        "    try:\n",
        "      sent = preprocess(sent)\n",
        "    except:\n",
        "      sent = \"\"\n",
        "\n",
        "  counts = [0,0]\n",
        "\n",
        "  split_sent = sent.split(\" \")\n",
        "  for word in split_sent:\n",
        "    if word in pos_words:\n",
        "      # print(\"+++{}+++\".format(word))\n",
        "      counts[0]+=1\n",
        "    if word in neg_words:\n",
        "      # print(\"---{}---\".format(word))\n",
        "      counts[1]+=1\n",
        "\n",
        "  score = counts[0]*weights[0]+counts[1]*weights[1]+weights[2]\n",
        "\n",
        "  if score>0:\n",
        "    return 1\n",
        "  elif score<0:\n",
        "    return -1\n",
        "  else:\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "X_bA5mFI6t2I"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(X, y, pos_words, neg_words, weights, preprocess=None):\n",
        "  pred = [get_score(sent, pos_words, neg_words, weights, preprocess) for sent in X]\n",
        "  correct, wrong = 0,0\n",
        "  for i in range(len(y)):\n",
        "    if pred[i]==y[i]:\n",
        "      correct+=1\n",
        "    else:\n",
        "      wrong+=1\n",
        "\n",
        "  ground_dict = {0:0, 1:0, -1:0}\n",
        "  pred_dict = {0:0, 1:0, -1:0}\n",
        "\n",
        "  for i in y:\n",
        "    ground_dict[i]+=1\n",
        "  for i in pred:\n",
        "    pred_dict[i]+=1\n",
        "\n",
        "  for i in [-1,0,1]:\n",
        "    print(i, ground_dict[i], pred_dict[i])\n",
        "\n",
        "  return (correct/(correct+wrong))*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5KLQkBWL8km5"
      },
      "outputs": [],
      "source": [
        "def get_sent_list(path):\n",
        "  X,y = [],[]\n",
        "  with open(path) as f:\n",
        "    for line in f:\n",
        "      label, sent = line.split(\"|||\")\n",
        "      label = int(label.strip())\n",
        "      sent = sent.strip()\n",
        "      X.append(sent)\n",
        "      y.append(label)\n",
        "\n",
        "  return X,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nUSh1blL-wec"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "X_train, y_train = get_sent_list(os.path.join(DATA_ROOT, \"train.txt\"))\n",
        "X_dev, y_dev = get_sent_list(os.path.join(DATA_ROOT, \"dev.txt\"))\n",
        "X_test, y_test = get_sent_list(os.path.join(DATA_ROOT, \"test.txt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9LHRXjc_oeI"
      },
      "outputs": [],
      "source": [
        "pos_words_1 = [\"good\", \"love\", \"enjoy\", \"nice\", \"amazing\"]\n",
        "neg_words_1 = [\"bad\", \"worst\", \"disappoint\", \"hate\", \"underwhelm\"]\n",
        "weights_1 = [1,1,0.5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap5--87mAJf2",
        "outputId": "56cc5039-866e-4a6a-fb98-8c1f9d358d7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-1 3310 0\n",
            "0 1624 0\n",
            "1 3610 8544\n",
            "42.25187265917603\n",
            "-1 912 0\n",
            "0 389 0\n",
            "1 909 2210\n",
            "41.13122171945701\n"
          ]
        }
      ],
      "source": [
        "print(get_accuracy(X_train, y_train, pos_words_1, neg_words_1, weights_1))\n",
        "print(get_accuracy(X_test, y_test, pos_words_1, neg_words_1, weights_1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjS04nqGAa6f"
      },
      "source": [
        "## Get more words from ChatGPT\n",
        "\n",
        "Add more pos and neg words from ChatGPT\n",
        "\n",
        "**Prompt used**: \"I am building a rule based sentiment analyser in Python. Give me a list of positive and negative words to be detected.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDLSV40qAUoo"
      },
      "outputs": [],
      "source": [
        "pos_words_2 = [\n",
        "    'happy',\n",
        "    'great',\n",
        "    'excellent',\n",
        "    'wonderful',\n",
        "    'amazing',\n",
        "    'fantastic',\n",
        "    'love',\n",
        "    'awesome',\n",
        "    'good',\n",
        "    'fantastic',\n",
        "    'incredible',\n",
        "    'fabulous',\n",
        "    'superb',\n",
        "    'delightful',\n",
        "    'charming'\n",
        "]\n",
        "\n",
        "neg_words_2 = [\n",
        "    'sad',\n",
        "    'bad',\n",
        "    'terrible',\n",
        "    'awful',\n",
        "    'horrible',\n",
        "    'dislike',\n",
        "    'hate',\n",
        "    'negative',\n",
        "    'disappointing',\n",
        "    'frustrating',\n",
        "    'annoying',\n",
        "    'unpleasant',\n",
        "    'lousy',\n",
        "    'pathetic',\n",
        "    'dreadful'\n",
        "]\n",
        "\n",
        "weights_2 = [1,1,0.5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYQu_4jUCHE5",
        "outputId": "ed3bb9fa-52fb-4c8c-9e38-4a02149a4b57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-1 3310 0\n",
            "0 1624 0\n",
            "1 3610 8544\n",
            "42.25187265917603\n",
            "-1 912 0\n",
            "0 389 0\n",
            "1 909 2210\n",
            "41.13122171945701\n"
          ]
        }
      ],
      "source": [
        "print(get_accuracy(X_train, y_train, pos_words_2, neg_words_2, weights_2))\n",
        "print(get_accuracy(X_test, y_test, pos_words_2, neg_words_2, weights_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeJvE4PnhBZv"
      },
      "outputs": [],
      "source": [
        "pos_words_3 = [\n",
        "    'happy',\n",
        "    'great',\n",
        "    'excellent',\n",
        "    'wonderful',\n",
        "    'amazing',\n",
        "    'fantastic',\n",
        "    'love',\n",
        "    'awesome',\n",
        "    'good',\n",
        "    'fantastic',\n",
        "    'incredible',\n",
        "    'fabulous',\n",
        "    'superb',\n",
        "    'delightful',\n",
        "    'charming'\n",
        "]\n",
        "\n",
        "neg_words_3 = [\n",
        "    'sad',\n",
        "    'bad',\n",
        "    'terrible',\n",
        "    'awful',\n",
        "    'horrible',\n",
        "    'dislike',\n",
        "    'hate',\n",
        "    'negative',\n",
        "    'disappointing',\n",
        "    'frustrating',\n",
        "    'annoying',\n",
        "    'unpleasant',\n",
        "    'lousy',\n",
        "    'pathetic',\n",
        "    'dreadful'\n",
        "]\n",
        "\n",
        "weights_3 = [1,1,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4KDPfXzhErF",
        "outputId": "d7bf73d0-c40d-4aff-a3ae-379ed7be0453"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-1 3310 0\n",
            "0 1624 7584\n",
            "1 3610 960\n",
            "22.495318352059925\n",
            "-1 912 0\n",
            "0 389 1971\n",
            "1 909 239\n",
            "20.13574660633484\n"
          ]
        }
      ],
      "source": [
        "print(get_accuracy(X_train, y_train, pos_words_3, neg_words_3, weights_3))\n",
        "print(get_accuracy(X_test, y_test, pos_words_3, neg_words_3, weights_3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrF0BB3mhjgb"
      },
      "source": [
        "## Add preprocesing\n",
        "\n",
        "Seems like all are getting mapped to a particular class, maybe preprocessing will help"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmE1g0Uzlq4g",
        "outputId": "c9c857c5-baee-4f4e-c3db-0d46e19105b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.7.2-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspellchecker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJIsmg20pfOW"
      },
      "source": [
        "## Big Dicts need to hide.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mLsIW7naloqb"
      },
      "outputs": [],
      "source": [
        "emotion_dict = {\n",
        "    \":)\": \"happy\",\n",
        "    \":D\": \"happy\",\n",
        "    \":]\": \"happy\",\n",
        "    \":(\": \"sad\",\n",
        "    \":'(\": \"sad\",\n",
        "    \":'[\": \"sad\",\n",
        "    \":/\": \"confused\",\n",
        "    \":|\": \"neutral\",\n",
        "    \":O\": \"surprised\",\n",
        "    \":*\": \"love\",\n",
        "    \":P\": \"playful\",\n",
        "    \";)\": \"winking\",\n",
        "    \":')\": \"tears of joy\",\n",
        "    \"<3\": \"heart\",\n",
        "    \":@\": \"angry\",\n",
        "    \":$\": \"embarrassed\",\n",
        "    \":S\": \"confused\",\n",
        "    \":\\\\\": \"confused\",\n",
        "    \":#\": \"silence\",\n",
        "    \":'D\": \"laughing\",\n",
        "    \"XD\": \"laughing\",\n",
        "    \"X-D\": \"laughing\",\n",
        "    \":|\": \"disappointed\",\n",
        "    \":>\": \"smug\",\n",
        "    \":-)\": \"happy\",\n",
        "    \":-D\": \"happy\",\n",
        "    \":-]\": \"happy\",\n",
        "    \":-(\": \"sad\",\n",
        "    \":'-(\": \"sad\",\n",
        "    \":'-[\": \"sad\",\n",
        "    \":-/\": \"confused\",\n",
        "    \":-|\": \"neutral\",\n",
        "    \":-O\": \"surprised\",\n",
        "    \":-*\": \"love\",\n",
        "    \":-P\": \"playful\",\n",
        "    \";-)\": \"winking\",\n",
        "    \":'-)\": \"tears of joy\",\n",
        "    \"<3\": \"heart\",\n",
        "    \":-@\": \"angry\",\n",
        "    \":-$\": \"embarrassed\",\n",
        "    \":-S\": \"confused\",\n",
        "    \":-\\\\\": \"confused\",\n",
        "    \":-#\": \"silence\",\n",
        "    \":'-D\": \"laughing\",\n",
        "    \"XD\": \"laughing\",\n",
        "    \"X-D\": \"laughing\",\n",
        "    \":-|\": \"disappointed\",\n",
        "    \":->\": \"smug\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zEKqAxZ8mUQj"
      },
      "outputs": [],
      "source": [
        "abbreviation_dict = {\n",
        "    \"lol\": \"laugh out loud\",\n",
        "    \"omg\": \"oh my god\",\n",
        "    \"brb\": \"be right back\",\n",
        "    \"btw\": \"by the way\",\n",
        "    \"idk\": \"I don't know\",\n",
        "    \"jk\": \"just kidding\",\n",
        "    \"tbh\": \"to be honest\",\n",
        "    \"gtg\": \"got to go\",\n",
        "    \"bff\": \"best friends forever\",\n",
        "    \"imo\": \"in my opinion\",\n",
        "    \"imho\": \"in my humble opinion\",\n",
        "    \"fyi\": \"for your information\",\n",
        "    \"np\": \"no problem\",\n",
        "    \"thx\": \"thanks\",\n",
        "    \"yw\": \"you're welcome\",\n",
        "    \"rofl\": \"rolling on the floor laughing\",\n",
        "    \"afk\": \"away from keyboard\",\n",
        "    \"irl\": \"in real life\",\n",
        "    \"nvm\": \"never mind\",\n",
        "    \"smh\": \"shaking my head\",\n",
        "    \"omw\": \"on my way\",\n",
        "    \"ikr\": \"I know, right?\",\n",
        "    \"tmi\": \"too much information\",\n",
        "    \"btwn\": \"between\",\n",
        "    \"wtf\": \"what the f***\",\n",
        "    \"ftw\": \"for the win\",\n",
        "    \"im\": \"instant message\",\n",
        "    \"dm\": \"direct message\",\n",
        "    \"np\": \"no problem\",\n",
        "    \"pls\": \"please\",\n",
        "    \"sry\": \"sorry\",\n",
        "    \"tho\": \"though\",\n",
        "    \"wth\": \"what the heck\",\n",
        "    \"oml\": \"oh my lord\",\n",
        "    \"ic\": \"I see\",\n",
        "    \"omd\": \"oh my days\",\n",
        "    \"ama\": \"ask me anything\",\n",
        "    \"hmu\": \"hit me up\",\n",
        "    \"rn\": \"right now\",\n",
        "    \"gg\": \"good game\",\n",
        "    \"fyf\": \"for your future\",\n",
        "    \"fomo\": \"fear of missing out\",\n",
        "    \"irl\": \"in real life\",\n",
        "    \"lmk\": \"let me know\",\n",
        "    \"nbd\": \"no big deal\",\n",
        "    \"omgosh\": \"oh my gosh\",\n",
        "    \"ttyl\": \"talk to you later\",\n",
        "    \"yolo\": \"you only live once\",\n",
        "    \"smh\": \"shaking my head\",\n",
        "    \"imo\": \"in my opinion\",\n",
        "    \"hth\": \"hope this helps\",\n",
        "    \"yw\": \"you're welcome\",\n",
        "    \"btw\": \"by the way\",\n",
        "    \"omw\": \"on my way\",\n",
        "    \"bff\": \"best friends forever\",\n",
        "    \"imo\": \"in my opinion\",\n",
        "    \"irl\": \"in real life\",\n",
        "    \"jk\": \"just kidding\",\n",
        "    \"lmao\": \"laughing my ass off\",\n",
        "    \"lmfao\": \"laughing my f***ing ass off\",\n",
        "    \"omfg\": \"oh my f***ing god\",\n",
        "    \"wtf\": \"what the f***\",\n",
        "    \"idc\": \"I don't care\",\n",
        "    \"idgaf\": \"I don't give a f***\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaXtWmGAhGJN",
        "outputId": "dbadd021-65e9-47ce-affe-8fc9d21e865e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "def lower_case(sent):\n",
        "  return sent.lower()\n",
        "\n",
        "def tokenize(sent):\n",
        "  return nltk.word_tokenize(sent)\n",
        "\n",
        "def rem_punc(sent):\n",
        "  return sent.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "def rem_stop_words(tokens):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "def rem_num_chars(tokens):\n",
        "  return [word for word in tokens if not word.isnumeric()]\n",
        "\n",
        "def stemm(tokens, lem_type=\"porter\"):\n",
        "  if lem_type==\"wordnet\":\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
        "  else:\n",
        "    stemmer = PorterStemmer()\n",
        "    return [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "\n",
        "def spell_check(tokens):\n",
        "  spell = SpellChecker()\n",
        "\n",
        "  corrected_tokens = []\n",
        "  for word in tokens:\n",
        "      corrected_word = spell.correction(word)\n",
        "      corrected_tokens.append(corrected_word)\n",
        "\n",
        "  return corrected_tokens\n",
        "\n",
        "def replace_emoticons(sent):\n",
        "  for emoticon, sentiment in emotion_dict.items():\n",
        "    sent = sent.replace(emoticon, sentiment)\n",
        "  return sent\n",
        "\n",
        "def replace_abbr(sent):\n",
        "  for abbreviation, expanded_form in abbreviation_dict.items():\n",
        "    sent = sent.replace(abbreviation, expanded_form)\n",
        "  return sent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfJvVPu-pi5x"
      },
      "source": [
        "## Restart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "R3V4rh8Hmny7"
      },
      "outputs": [],
      "source": [
        "apply_order = [lower_case, replace_emoticons, replace_abbr, rem_punc, \\\n",
        "               tokenize, rem_stop_words, rem_num_chars, spell_check, stemm]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d3MtN54p87D",
        "outputId": "6ebb663d-64a0-4cc3-e10a-d57525bedfb9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['love', 'film', 'love', 'perform', 'buy', 'actor']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "sent = \"\"\"It 's a lovely film with lovely performances by Buy and Accorsi .\"\"\"\n",
        "\n",
        "for func in apply_order:\n",
        "  sent = func(sent)\n",
        "\n",
        "sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0X0Z1PgKqeXs"
      },
      "outputs": [],
      "source": [
        "pos_words = [\n",
        "    'happy',\n",
        "    'great',\n",
        "    'excellent',\n",
        "    'wonderful',\n",
        "    'amazing',\n",
        "    'fantastic',\n",
        "    'love',\n",
        "    'awesome',\n",
        "    'good',\n",
        "    'fantastic',\n",
        "    'incredible',\n",
        "    'fabulous',\n",
        "    'superb',\n",
        "    'delightful',\n",
        "    'charming'\n",
        "]\n",
        "\n",
        "neg_words = [\n",
        "    'sad',\n",
        "    'bad',\n",
        "    'terrible',\n",
        "    'awful',\n",
        "    'horrible',\n",
        "    'dislike',\n",
        "    'hate',\n",
        "    'negative',\n",
        "    'disappointing',\n",
        "    'frustrating',\n",
        "    'annoying',\n",
        "    'unpleasant',\n",
        "    'lousy',\n",
        "    'pathetic',\n",
        "    'dreadful'\n",
        "]\n",
        "\n",
        "preprocess_words = [spell_check, stemm]\n",
        "\n",
        "for func in preprocess_words:\n",
        "  pos_words = func(pos_words)\n",
        "  neg_words = func(neg_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SF-bAOQYtJlV",
        "outputId": "102b7857-8f9d-40ba-d3a8-8a2a5b17af5c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['happi',\n",
              " 'great',\n",
              " 'excel',\n",
              " 'wonder',\n",
              " 'amaz',\n",
              " 'fantast',\n",
              " 'love',\n",
              " 'awesom',\n",
              " 'good',\n",
              " 'fantast',\n",
              " 'incred',\n",
              " 'fabul',\n",
              " 'superb',\n",
              " 'delight',\n",
              " 'charm']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "pos_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylRH4-yltLlV",
        "outputId": "475fbdbf-a0f3-408f-e090-b4952813a428"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sad',\n",
              " 'bad',\n",
              " 'terribl',\n",
              " 'aw',\n",
              " 'horribl',\n",
              " 'dislik',\n",
              " 'hate',\n",
              " 'neg',\n",
              " 'disappoint',\n",
              " 'frustrat',\n",
              " 'annoy',\n",
              " 'unpleas',\n",
              " 'lousi',\n",
              " 'pathet',\n",
              " 'dread']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "neg_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "IjbBxbcetU7H"
      },
      "outputs": [],
      "source": [
        "def preprocess(x):\n",
        "  for func in apply_order:\n",
        "    x = func(x)\n",
        "\n",
        "  return \" \".join(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "1XVb9JJ7tuPA"
      },
      "outputs": [],
      "source": [
        "weights = [1,-1,0.5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9sXp1DdtPOa",
        "outputId": "d06bd069-a363-496e-9c19-299fc6c2ac16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1 3310 219\n",
            "0 1624 7808\n",
            "1 3610 517\n",
            "23.057116104868914\n",
            "-1 912 52\n",
            "0 389 2008\n",
            "1 909 150\n",
            "21.49321266968326\n"
          ]
        }
      ],
      "source": [
        "print(get_accuracy(X_train, y_train, pos_words, neg_words, weights, preprocess))\n",
        "print(get_accuracy(X_test, y_test, pos_words, neg_words, weights, preprocess))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_accuracy(X_train, y_train, pos_words, neg_words, weights, preprocess))\n",
        "print(get_accuracy(X_test, y_test, pos_words, neg_words, weights, preprocess))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99hXzuMrrAmf",
        "outputId": "de74ae3d-ded8-45e6-d589-34d02a2c8781"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1 3310 219\n",
            "0 1624 0\n",
            "1 3610 8325\n",
            "43.445692883895134\n",
            "-1 912 52\n",
            "0 389 0\n",
            "1 909 2158\n",
            "42.262443438914026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2kpnmaLuJ1w"
      },
      "outputs": [],
      "source": [
        "test_sent = \"Here 's yet another studio horror franchise mucking up its storyline with glitches casual fans could correct in their sleep .\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYNjmpl8uHbS",
        "outputId": "aba61a6d-6f3a-4388-85dd-09aea74215d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_score(test_sent, pos_words, neg_words, weights, preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "S1rUHiERvGsK",
        "outputId": "7aef0608-3b01-4907-d616-52e1b9371066"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'love film love perform buy actor'"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preprocess(test_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yE1yqQVfu0lh"
      },
      "outputs": [],
      "source": [
        "test_sent = \"It 's a lovely film with lovely performances by Buy and Accorsi .\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQTC9/RT9Wwp3FqqST4HUT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}