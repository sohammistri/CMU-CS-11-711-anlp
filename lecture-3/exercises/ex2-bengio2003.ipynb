{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "070f22ee",
   "metadata": {},
   "source": [
    "# Logs\n",
    "## No weight sharing + direct\n",
    "- `embedding_dim`: 32\n",
    "- `window_size`: 4\n",
    "- Train Metrics: Train Loss: 5.4947, Train Entropy: 7.9272, Train Perplexity: 243.41\n",
    "- Val Metrics: Val Loss: 6.4856, Val Entropy: 9.3567, Val Perplexity: 655.61\n",
    "- Test Metrics: Test Loss: 6.6741, Test Entropy: 9.6287, Test Perplexity: 791.63\n",
    "- Tiny Shakespeare Metrics: Test Loss: 7.4938, Test Entropy: 10.8113, Test Perplexity: 1796.85\n",
    "\n",
    "## Weight sharing + direct\n",
    "- `embedding_dim`: 32\n",
    "- `window_size`: 4\n",
    "- Train Metrics: Train Loss: 5.5120, Train Entropy: 7.9521, Train Perplexity: 247.64\n",
    "- Val Metrics: Val Loss: 6.6535, Val Entropy: 9.5990, Val Perplexity: 775.52\n",
    "- Test Metrics: Test Loss: 6.8656, Test Entropy: 9.9050, Test Perplexity: 958.76\n",
    "- Tiny Shakespeare Metrics: Test Loss: 7.6186, Test Entropy: 10.9913, Test Perplexity: 2035.64\n",
    "\n",
    "## Weight sharing + direct (best model)\n",
    "- `embedding_dim`: 128\n",
    "- `window_size`: 16\n",
    "- Train Metrics: Train Loss: 4.7928, Train Entropy: 6.9146, Train Perplexity: 120.64\n",
    "- Val Metrics: Val Loss: 6.6115, Val Entropy: 9.5384, Val Perplexity: 743.59\n",
    "- Test Metrics: Test Loss: 6.8086, Test Entropy: 9.8228, Test Perplexity: 905.62\n",
    "- Tiny Shakespeare Metrics: Test Loss: 7.4555, Test Entropy: 10.7560, Test Perplexity: 1729.38"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38013042",
   "metadata": {},
   "source": [
    "# Implement Neural Probabilistic LM (Bengio et. al 2003)\n",
    "\n",
    "Implement the classic Neural LM paper using torch trained on 800K FineWeb Edu Tokens. We will implement more things like direct networks, weight tying to this model and report Per token NLL, Entropy and Perplexity on our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6d3a8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/CMU-CS-11-711-anlp/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8fe89c6b50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import heapq\n",
    "import random\n",
    "from IPython.display import display, Markdown\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4498981c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d18fea",
   "metadata": {},
   "source": [
    "# 1. Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e76cc120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_from_files(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        str_tokens = f.read().splitlines()\n",
    "        tokens = [int(token) for token in str_tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc6f9e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = load_dataset_from_files(\"train_tokens.txt\")\n",
    "val_tokens = load_dataset_from_files(\"val_tokens.txt\")\n",
    "test_tokens = load_dataset_from_files(\"test_tokens.txt\")\n",
    "ts_tokens = load_dataset_from_files(\"ts_tokens.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5aaf3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800258, 100033, 100032, 338025)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tokens), len(test_tokens), len(val_tokens), len(ts_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e609fde8",
   "metadata": {},
   "source": [
    "# 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c558cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(tokens, context_window_size):\n",
    "    x, y = [], []\n",
    "\n",
    "    for i in range(len(tokens) - context_window_size):\n",
    "        x.append(tokens[i : i + context_window_size])\n",
    "        y.append(tokens[i + context_window_size])\n",
    "\n",
    "    x = torch.LongTensor(x)\n",
    "    y = torch.LongTensor(y)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0f7a080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model, tokens, context_window_size):\n",
    "    x_tensor, y_tensor = prepare_dataset(tokens, context_window_size)\n",
    "    dataset = TensorDataset(x_tensor, y_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=4096, shuffle=True, drop_last=False)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # get initial metrics\n",
    "    model.eval()\n",
    "    tmp_loss = 0.0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device) # (B, T), (B, )\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        tmp_loss += loss.item() * (x.shape[0] / x_tensor.shape[0])\n",
    "\n",
    "    perplexity = float(np.exp(tmp_loss))\n",
    "    entropy = float(np.log2(perplexity))\n",
    "\n",
    "    return tmp_loss, entropy, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b045378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_tokens, val_tokens, batch_size, num_epochs, lr, context_window_size):\n",
    "    x_train, y_train = prepare_dataset(train_tokens, context_window_size)\n",
    "    x_val, y_val = prepare_dataset(val_tokens, context_window_size)\n",
    "\n",
    "    train_dataset = TensorDataset(x_train, y_train)\n",
    "    val_dataset = TensorDataset(x_val, y_val)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    metrics = {\"NLL\": [], \"Entropy\": [], \"Perplexity\": []}\n",
    "\n",
    "    # get initial metrics\n",
    "    model.eval()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    for x, y in train_dataloader:\n",
    "        x, y = x.to(device), y.to(device) # (B, T), (B, )\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        train_loss += loss.item() * (x.shape[0] / x_train.shape[0])\n",
    "\n",
    "    train_perplexity = float(np.exp(train_loss))\n",
    "    train_entropy = float(np.log2(train_perplexity))\n",
    "\n",
    "    # eval\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    for x, y in val_dataloader:\n",
    "        x, y = x.to(device), y.to(device) # (B, T), (B, )\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        val_loss += loss.item() * (x.shape[0] / x_val.shape[0])\n",
    "\n",
    "    val_perplexity = float(np.exp(val_loss))\n",
    "    val_entropy = float(np.log2(val_perplexity))\n",
    "\n",
    "    metrics[\"NLL\"].append((train_loss, val_loss))\n",
    "    metrics[\"Entropy\"].append((train_entropy, val_entropy))\n",
    "    metrics[\"Perplexity\"].append((train_perplexity, val_perplexity))\n",
    "\n",
    "    print(f\"Start of training: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "\n",
    "    # now start training\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # train loop\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for x, y in train_dataloader:\n",
    "            x, y = x.to(device), y.to(device) # (B, T), (B, )\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * (x.shape[0] / x_train.shape[0])\n",
    "\n",
    "        train_perplexity = float(np.exp(train_loss))\n",
    "        train_entropy = float(np.log2(train_perplexity))\n",
    "\n",
    "        # eval\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        for x, y in val_dataloader:\n",
    "            x, y = x.to(device), y.to(device) # (B, T), (B, )\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item() * (x.shape[0] / x_val.shape[0])\n",
    "\n",
    "        val_perplexity = float(np.exp(val_loss))\n",
    "        val_entropy = float(np.log2(val_perplexity))\n",
    "\n",
    "        metrics[\"NLL\"].append((train_loss, val_loss))\n",
    "        metrics[\"Entropy\"].append((train_entropy, val_entropy))\n",
    "        metrics[\"Perplexity\"].append((train_perplexity, val_perplexity))\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da810db2",
   "metadata": {},
   "source": [
    "# 3. Write Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40914cfc",
   "metadata": {},
   "source": [
    "## 3.1 Standard Model descibed in paper\n",
    "\n",
    "No weight tying yet, will add options to have direct connections.\n",
    "\n",
    "**Paper formula**: $y = b + Wx + U \\text{tanh}(d + Hx)$, here $y$ are the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e8b2f184",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLM(nn.Module):\n",
    "    def __init__(self, vocab_size, context_window_size, embedding_dim=32, direct=False):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.context_window_size = context_window_size\n",
    "        self.direct = direct\n",
    "\n",
    "        # model params\n",
    "        self.C = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim * context_window_size, embedding_dim)\n",
    "        self.linear2 = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.W = None\n",
    "\n",
    "        if direct:\n",
    "            self.W = nn.Parameter(torch.zeros(embedding_dim * context_window_size, vocab_size))\n",
    "\n",
    "        # initialize by Xaview init\n",
    "        nn.init.xavier_uniform_(self.C.weight)\n",
    "        nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        nn.init.zeros_(self.linear1.bias)\n",
    "        nn.init.xavier_uniform_(self.linear2.weight)\n",
    "        nn.init.zeros_(self.linear2.bias)\n",
    "        if direct:\n",
    "            nn.init.xavier_uniform_(self.W.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, context_window)\n",
    "        embeddings = self.C(x) # (B, T, C)\n",
    "        B, T, C = embeddings.shape\n",
    "        model_inp = embeddings.view(B, T * C)\n",
    "        hidden = self.linear1(model_inp) # (B, C)\n",
    "        hidden = torch.tanh(hidden)\n",
    "        logits = self.linear2(hidden) # (B, V)\n",
    "        if self.direct:\n",
    "            logits += model_inp @ self.W # (B, T*C) @ (T*C, V)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "619af5ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = max(train_tokens) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8f3753",
   "metadata": {},
   "source": [
    "### Exp.1: Small model and window size (no direct)\n",
    "\n",
    "- `embedding_dim`: 32\n",
    "- `window_size`: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3bf0fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 32\n",
    "context_window_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralLM(vocab_size=vocab_size, context_window_size=context_window_size, embedding_dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (C): Embedding(50257, 32)\n",
       "  (linear1): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (linear2): Linear(in_features=32, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fef0dda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in model1: 3270833\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters in model1: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7045280c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of training: Train Loss: 10.8249, Val Loss: 10.8249\n",
      "Epoch 1: Train Loss: 8.6384, Val Loss: 7.6631\n",
      "Epoch 2: Train Loss: 7.4912, Val Loss: 7.6584\n",
      "Epoch 3: Train Loss: 7.4666, Val Loss: 7.6405\n",
      "Epoch 4: Train Loss: 7.4077, Val Loss: 7.5621\n",
      "Epoch 5: Train Loss: 7.3126, Val Loss: 7.4816\n",
      "Epoch 6: Train Loss: 7.1956, Val Loss: 7.3901\n",
      "Epoch 7: Train Loss: 7.0675, Val Loss: 7.2873\n",
      "Epoch 8: Train Loss: 6.9275, Val Loss: 7.1791\n",
      "Epoch 9: Train Loss: 6.7886, Val Loss: 7.0793\n",
      "Epoch 10: Train Loss: 6.6435, Val Loss: 6.9818\n",
      "Epoch 11: Train Loss: 6.5038, Val Loss: 6.9001\n",
      "Epoch 12: Train Loss: 6.3709, Val Loss: 6.8273\n",
      "Epoch 13: Train Loss: 6.2514, Val Loss: 6.7712\n",
      "Epoch 14: Train Loss: 6.1477, Val Loss: 6.7324\n",
      "Epoch 15: Train Loss: 6.0582, Val Loss: 6.7057\n",
      "Epoch 16: Train Loss: 5.9794, Val Loss: 6.6854\n",
      "Epoch 17: Train Loss: 5.9077, Val Loss: 6.6740\n",
      "Epoch 18: Train Loss: 5.8416, Val Loss: 6.6658\n",
      "Epoch 19: Train Loss: 5.7797, Val Loss: 6.6609\n",
      "Epoch 20: Train Loss: 5.7215, Val Loss: 6.6586\n"
     ]
    }
   ],
   "source": [
    "metrics1 = train(model, train_tokens, val_tokens, batch_size=4096, num_epochs=20, lr=1e-3, context_window_size=context_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d31368e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metrics: Train Loss: 5.7215, Train Entropy: 8.2544, Train Perplexity: 305.37\n",
      "Val Metrics: Val Loss: 6.6586, Val Entropy: 9.6063, Val Perplexity: 779.44\n"
     ]
    }
   ],
   "source": [
    "print(f'Train Metrics: Train Loss: {metrics1[\"NLL\"][-1][0]:.4f}, Train Entropy: {metrics1[\"Entropy\"][-1][0]:.4f}, Train Perplexity: {metrics1[\"Perplexity\"][-1][0]:.2f}')\n",
    "print(f'Val Metrics: Val Loss: {metrics1[\"NLL\"][-1][1]:.4f}, Val Entropy: {metrics1[\"Entropy\"][-1][1]:.4f}, Val Perplexity: {metrics1[\"Perplexity\"][-1][1]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cd64d8",
   "metadata": {},
   "source": [
    "### Exp.2: Larger model and window size (no direct)\n",
    "\n",
    "- `embedding_dim`: 64\n",
    "- `window_size`: 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40b0b9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "context_window_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d83fbb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralLM(vocab_size=vocab_size, context_window_size=context_window_size, embedding_dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec486eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (C): Embedding(50257, 64)\n",
       "  (linear1): Linear(in_features=512, out_features=64, bias=True)\n",
       "  (linear2): Linear(in_features=64, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fcf48365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in model1: 6515985\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters in model1: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "97d6ba38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of training: Train Loss: 5.8336, Val Loss: 6.6895\n",
      "Epoch 1: Train Loss: 5.8827, Val Loss: 6.7172\n",
      "Epoch 2: Train Loss: 5.7449, Val Loss: 6.7143\n",
      "Epoch 3: Train Loss: 5.6321, Val Loss: 6.7139\n",
      "Epoch 4: Train Loss: 5.5250, Val Loss: 6.7218\n",
      "Epoch 5: Train Loss: 5.4229, Val Loss: 6.7390\n"
     ]
    }
   ],
   "source": [
    "metrics1 = train(model, train_tokens, val_tokens, batch_size=4096, num_epochs=5, lr=1e-3, context_window_size=context_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c498921f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metrics: Train Loss: 5.4229, Train Entropy: 7.8236, Train Perplexity: 226.54\n",
      "Val Metrics: Val Loss: 6.7390, Val Entropy: 9.7223, Val Perplexity: 844.69\n"
     ]
    }
   ],
   "source": [
    "print(f'Train Metrics: Train Loss: {metrics1[\"NLL\"][-1][0]:.4f}, Train Entropy: {metrics1[\"Entropy\"][-1][0]:.4f}, Train Perplexity: {metrics1[\"Perplexity\"][-1][0]:.2f}')\n",
    "print(f'Val Metrics: Val Loss: {metrics1[\"NLL\"][-1][1]:.4f}, Val Entropy: {metrics1[\"Entropy\"][-1][1]:.4f}, Val Perplexity: {metrics1[\"Perplexity\"][-1][1]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b369f4e5",
   "metadata": {},
   "source": [
    "### Exp.3: Largest model and window size (no direct)\n",
    "\n",
    "- `embedding_dim`: 128\n",
    "- `window_size`: 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d47c49dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "context_window_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "96a6db31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralLM(vocab_size=vocab_size, context_window_size=context_window_size, embedding_dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a6f54a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (C): Embedding(50257, 128)\n",
       "  (linear1): Linear(in_features=2048, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e766f002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in model1: 13178321\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters in model1: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "19a20325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of training: Train Loss: 10.8249, Val Loss: 10.8249\n",
      "Epoch 1: Train Loss: 7.9763, Val Loss: 7.6983\n",
      "Epoch 2: Train Loss: 7.4525, Val Loss: 7.4868\n",
      "Epoch 3: Train Loss: 7.1273, Val Loss: 7.2155\n",
      "Epoch 4: Train Loss: 6.7672, Val Loss: 6.9607\n",
      "Epoch 5: Train Loss: 6.4054, Val Loss: 6.7589\n",
      "Epoch 6: Train Loss: 6.0837, Val Loss: 6.6781\n",
      "Epoch 7: Train Loss: 5.8200, Val Loss: 6.6634\n",
      "Epoch 8: Train Loss: 5.5889, Val Loss: 6.6906\n",
      "Epoch 9: Train Loss: 5.3728, Val Loss: 6.7393\n",
      "Epoch 10: Train Loss: 5.1656, Val Loss: 6.8131\n"
     ]
    }
   ],
   "source": [
    "metrics1 = train(model, train_tokens, val_tokens, batch_size=4096, num_epochs=10, lr=1e-3, context_window_size=context_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6bb599ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metrics: Train Loss: 5.1656, Train Entropy: 7.4524, Train Perplexity: 175.15\n",
      "Val Metrics: Val Loss: 6.8131, Val Entropy: 9.8292, Val Perplexity: 909.64\n"
     ]
    }
   ],
   "source": [
    "print(f'Train Metrics: Train Loss: {metrics1[\"NLL\"][-1][0]:.4f}, Train Entropy: {metrics1[\"Entropy\"][-1][0]:.4f}, Train Perplexity: {metrics1[\"Perplexity\"][-1][0]:.2f}')\n",
    "print(f'Val Metrics: Val Loss: {metrics1[\"NLL\"][-1][1]:.4f}, Val Entropy: {metrics1[\"Entropy\"][-1][1]:.4f}, Val Perplexity: {metrics1[\"Perplexity\"][-1][1]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9db33f",
   "metadata": {},
   "source": [
    "### Exp.4: Small model and window size with direct\n",
    "\n",
    "- `embedding_dim`: 32\n",
    "- `window_size`: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "46959608",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 32\n",
    "context_window_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6df21831",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralLM(vocab_size=vocab_size, context_window_size=context_window_size, embedding_dim=embedding_dim, direct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6614072a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (C): Embedding(50257, 32)\n",
       "  (linear1): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (linear2): Linear(in_features=32, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7f772db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in model1: 9703729\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters in model1: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "478328a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of training: Train Loss: 10.8249, Val Loss: 10.8249\n",
      "Epoch 1: Train Loss: 8.5126, Val Loss: 7.6690\n",
      "Epoch 2: Train Loss: 7.4406, Val Loss: 7.5469\n",
      "Epoch 3: Train Loss: 7.1953, Val Loss: 7.2398\n",
      "Epoch 4: Train Loss: 6.8058, Val Loss: 6.9385\n",
      "Epoch 5: Train Loss: 6.4589, Val Loss: 6.7549\n",
      "Epoch 6: Train Loss: 6.1971, Val Loss: 6.6487\n",
      "Epoch 7: Train Loss: 5.9868, Val Loss: 6.5780\n",
      "Epoch 8: Train Loss: 5.8043, Val Loss: 6.5305\n",
      "Epoch 9: Train Loss: 5.6416, Val Loss: 6.4999\n",
      "Epoch 10: Train Loss: 5.4947, Val Loss: 6.4856\n"
     ]
    }
   ],
   "source": [
    "metrics1 = train(model, train_tokens, val_tokens, batch_size=4096, num_epochs=10, lr=1e-3, context_window_size=context_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1e188c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metrics: Train Loss: 5.4947, Train Entropy: 7.9272, Train Perplexity: 243.41\n",
      "Val Metrics: Val Loss: 6.4856, Val Entropy: 9.3567, Val Perplexity: 655.61\n"
     ]
    }
   ],
   "source": [
    "print(f'Train Metrics: Train Loss: {metrics1[\"NLL\"][-1][0]:.4f}, Train Entropy: {metrics1[\"Entropy\"][-1][0]:.4f}, Train Perplexity: {metrics1[\"Perplexity\"][-1][0]:.2f}')\n",
    "print(f'Val Metrics: Val Loss: {metrics1[\"NLL\"][-1][1]:.4f}, Val Entropy: {metrics1[\"Entropy\"][-1][1]:.4f}, Val Perplexity: {metrics1[\"Perplexity\"][-1][1]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c14ae0",
   "metadata": {},
   "source": [
    "### Exp.5: Larger model and window size with direct\n",
    "\n",
    "- `embedding_dim`: 64\n",
    "- `window_size`: 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "80d8b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "context_window_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "42614237",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralLM(vocab_size=vocab_size, context_window_size=context_window_size, embedding_dim=embedding_dim, direct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a0b8ef52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (C): Embedding(50257, 64)\n",
       "  (linear1): Linear(in_features=512, out_features=64, bias=True)\n",
       "  (linear2): Linear(in_features=64, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "44d60782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in model1: 32247569\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters in model1: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "575d8823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of training: Train Loss: 10.8249, Val Loss: 10.8249\n",
      "Epoch 1: Train Loss: 8.1273, Val Loss: 7.6341\n",
      "Epoch 2: Train Loss: 7.2408, Val Loss: 7.1756\n",
      "Epoch 3: Train Loss: 6.6422, Val Loss: 6.7916\n",
      "Epoch 4: Train Loss: 6.1152, Val Loss: 6.5969\n",
      "Epoch 5: Train Loss: 5.6824, Val Loss: 6.5117\n"
     ]
    }
   ],
   "source": [
    "metrics1 = train(model, train_tokens, val_tokens, batch_size=4096, num_epochs=5, lr=1e-3, context_window_size=context_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d413d66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 50257])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.W.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d5c1d435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metrics: Train Loss: 5.6824, Train Entropy: 8.1979, Train Perplexity: 293.64\n",
      "Val Metrics: Val Loss: 6.5117, Val Entropy: 9.3944, Val Perplexity: 672.98\n"
     ]
    }
   ],
   "source": [
    "print(f'Train Metrics: Train Loss: {metrics1[\"NLL\"][-1][0]:.4f}, Train Entropy: {metrics1[\"Entropy\"][-1][0]:.4f}, Train Perplexity: {metrics1[\"Perplexity\"][-1][0]:.2f}')\n",
    "print(f'Val Metrics: Val Loss: {metrics1[\"NLL\"][-1][1]:.4f}, Val Entropy: {metrics1[\"Entropy\"][-1][1]:.4f}, Val Perplexity: {metrics1[\"Perplexity\"][-1][1]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605f8fe0",
   "metadata": {},
   "source": [
    "### Exp.6: Largest model and window size with direct\n",
    "\n",
    "- `embedding_dim`: 128\n",
    "- `window_size`: 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "23157351",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "context_window_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8e1e08a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralLM(vocab_size=vocab_size, context_window_size=context_window_size, embedding_dim=embedding_dim, direct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "97d1b066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (C): Embedding(50257, 128)\n",
       "  (linear1): Linear(in_features=2048, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "630dd2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in model1: 116104657\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters in model1: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ebb94a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of training: Train Loss: 5.6615, Val Loss: 6.6814\n",
      "Epoch 1: Train Loss: 5.6508, Val Loss: 6.6730\n",
      "Epoch 2: Train Loss: 5.4276, Val Loss: 6.6537\n",
      "Epoch 3: Train Loss: 5.2219, Val Loss: 6.6488\n",
      "Epoch 4: Train Loss: 5.0183, Val Loss: 6.6556\n",
      "Epoch 5: Train Loss: 4.8171, Val Loss: 6.6736\n"
     ]
    }
   ],
   "source": [
    "metrics1 = train(model, train_tokens, val_tokens, batch_size=4096, num_epochs=5, lr=2e-4, context_window_size=context_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "684f2436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 50257])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.W.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ea896100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metrics: Train Loss: 4.8171, Train Entropy: 6.9496, Train Perplexity: 123.61\n",
      "Val Metrics: Val Loss: 6.6736, Val Entropy: 9.6279, Val Perplexity: 791.20\n"
     ]
    }
   ],
   "source": [
    "print(f'Train Metrics: Train Loss: {metrics1[\"NLL\"][-1][0]:.4f}, Train Entropy: {metrics1[\"Entropy\"][-1][0]:.4f}, Train Perplexity: {metrics1[\"Perplexity\"][-1][0]:.2f}')\n",
    "print(f'Val Metrics: Val Loss: {metrics1[\"NLL\"][-1][1]:.4f}, Val Entropy: {metrics1[\"Entropy\"][-1][1]:.4f}, Val Perplexity: {metrics1[\"Perplexity\"][-1][1]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9453a878",
   "metadata": {},
   "source": [
    "### Final: Best Model Test and Tiny Shakespeare metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e09d08da",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 32\n",
    "context_window_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "883c6994",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralLM(vocab_size=vocab_size, context_window_size=context_window_size, embedding_dim=embedding_dim, direct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "75fc908d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (C): Embedding(50257, 32)\n",
       "  (linear1): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (linear2): Linear(in_features=32, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "70331857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in model1: 9703729\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters in model1: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3a73f95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of training: Train Loss: 10.8249, Val Loss: 10.8249\n",
      "Epoch 1: Train Loss: 8.5179, Val Loss: 7.6688\n",
      "Epoch 2: Train Loss: 7.4371, Val Loss: 7.5399\n",
      "Epoch 3: Train Loss: 7.1770, Val Loss: 7.2171\n",
      "Epoch 4: Train Loss: 6.7887, Val Loss: 6.9337\n",
      "Epoch 5: Train Loss: 6.4530, Val Loss: 6.7488\n",
      "Epoch 6: Train Loss: 6.1891, Val Loss: 6.6372\n",
      "Epoch 7: Train Loss: 5.9773, Val Loss: 6.5692\n",
      "Epoch 8: Train Loss: 5.7960, Val Loss: 6.5225\n",
      "Epoch 9: Train Loss: 5.6355, Val Loss: 6.4957\n",
      "Epoch 10: Train Loss: 5.4919, Val Loss: 6.4812\n"
     ]
    }
   ],
   "source": [
    "metrics1 = train(model, train_tokens, val_tokens, batch_size=4096, num_epochs=10, lr=1e-3, context_window_size=context_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metrics: Train Loss: 5.3550, Train Entropy: 7.7256, Train Perplexity: 211.66\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_entropy, train_perplexity = get_metrics(model, train_tokens, context_window_size)\n",
    "print(f'Train Metrics: Train Loss: {train_loss:.4f}, Train Entropy: {train_entropy:.4f}, Train Perplexity: {train_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1c3fbb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Metrics: Val Loss: 6.4812, Val Entropy: 9.3504, Val Perplexity: 652.73\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_entropy, val_perplexity = get_metrics(model, val_tokens, context_window_size)\n",
    "print(f'Val Metrics: Val Loss: {val_loss:.4f}, Val Entropy: {val_entropy:.4f}, Val Perplexity: {val_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "134ccf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics: Test Loss: 6.6741, Test Entropy: 9.6287, Test Perplexity: 791.63\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_entropy, test_perplexity = get_metrics(model, test_tokens, context_window_size)\n",
    "print(f'Test Metrics: Test Loss: {test_loss:.4f}, Test Entropy: {test_entropy:.4f}, Test Perplexity: {test_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "dc951678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiny Shakespeare Metrics: Test Loss: 7.4938, Test Entropy: 10.8113, Test Perplexity: 1796.85\n"
     ]
    }
   ],
   "source": [
    "ts_loss, ts_entropy, ts_perplexity = get_metrics(model, ts_tokens, context_window_size)\n",
    "print(f'Tiny Shakespeare Metrics: Test Loss: {ts_loss:.4f}, Test Entropy: {ts_entropy:.4f}, Test Perplexity: {ts_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90051cc",
   "metadata": {},
   "source": [
    "## 3.2 Weight tying\n",
    "\n",
    "Implement weight tying where output layer embeddings and input layer embedding matrix are forecefully tied\n",
    "\n",
    "**Paper formula**: $y = b + Wx + U \\text{tanh}(d + Hx)$, here $y$ are the logits. \n",
    "\n",
    "In weight tying, we **force** $C = U$ where $C$ is input embedding matrix and $U$ is the output matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "574dcc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLMWeightTied(nn.Module):\n",
    "    def __init__(self, vocab_size, context_window_size, embedding_dim=32, direct=False):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.context_window_size = context_window_size\n",
    "        self.direct = direct\n",
    "\n",
    "        # model params\n",
    "        self.C = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim * context_window_size, embedding_dim)\n",
    "        self.linear2 = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.W = None\n",
    "\n",
    "        if direct:\n",
    "            self.W = nn.Parameter(torch.zeros(embedding_dim * context_window_size, vocab_size))\n",
    "\n",
    "        # initialize by Xaview init\n",
    "        nn.init.xavier_uniform_(self.C.weight)\n",
    "        nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        nn.init.zeros_(self.linear1.bias)\n",
    "        # force weight tying\n",
    "        self.C.weight = self.linear2.weight\n",
    "        nn.init.zeros_(self.linear2.bias)\n",
    "        if direct:\n",
    "            nn.init.xavier_uniform_(self.W.data)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, context_window)\n",
    "        embeddings = self.C(x) # (B, T, C)\n",
    "        B, T, C = embeddings.shape\n",
    "        model_inp = embeddings.view(B, T * C)\n",
    "        hidden = self.linear1(model_inp) # (B, C)\n",
    "        hidden = torch.tanh(hidden)\n",
    "        logits = self.linear2(hidden) # (B, V)\n",
    "        if self.direct:\n",
    "            logits += model_inp @ self.W # (B, T*C) @ (T*C, V)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcb765cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = max(train_tokens) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7a7a77",
   "metadata": {},
   "source": [
    "### Exp.1: Small model and window size with direct\n",
    "\n",
    "- `embedding_dim`: 32\n",
    "- `window_size`: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2cf3025",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 32\n",
    "context_window_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "961ab4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralLMWeightTied(vocab_size=vocab_size, context_window_size=context_window_size, embedding_dim=embedding_dim, direct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "acd3c7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLMWeightTied(\n",
       "  (C): Embedding(50257, 32)\n",
       "  (linear1): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (linear2): Linear(in_features=32, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07d14620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in model1: 8095505\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters in model1: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc3664f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of training: Train Loss: 10.8258, Val Loss: 10.8256\n",
      "Epoch 1: Train Loss: 8.6739, Val Loss: 7.7258\n",
      "Epoch 2: Train Loss: 7.3059, Val Loss: 7.4537\n",
      "Epoch 3: Train Loss: 6.9948, Val Loss: 7.2189\n",
      "Epoch 4: Train Loss: 6.7170, Val Loss: 7.0339\n",
      "Epoch 5: Train Loss: 6.4672, Val Loss: 6.8960\n",
      "Epoch 6: Train Loss: 6.2485, Val Loss: 6.7964\n",
      "Epoch 7: Train Loss: 6.0591, Val Loss: 6.7305\n",
      "Epoch 8: Train Loss: 5.8936, Val Loss: 6.6876\n",
      "Epoch 9: Train Loss: 5.7464, Val Loss: 6.6624\n",
      "Epoch 10: Train Loss: 5.6138, Val Loss: 6.6485\n"
     ]
    }
   ],
   "source": [
    "metrics1 = train(model, train_tokens, val_tokens, batch_size=4096, num_epochs=10, lr=1e-3, context_window_size=context_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "30b607ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.C.weight.grad == model.linear2.weight.grad).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "770e3dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metrics: Train Loss: 5.6138, Train Entropy: 8.0990, Train Perplexity: 274.18\n",
      "Val Metrics: Val Loss: 6.6485, Val Entropy: 9.5918, Val Perplexity: 771.64\n"
     ]
    }
   ],
   "source": [
    "print(f'Train Metrics: Train Loss: {metrics1[\"NLL\"][-1][0]:.4f}, Train Entropy: {metrics1[\"Entropy\"][-1][0]:.4f}, Train Perplexity: {metrics1[\"Perplexity\"][-1][0]:.2f}')\n",
    "print(f'Val Metrics: Val Loss: {metrics1[\"NLL\"][-1][1]:.4f}, Val Entropy: {metrics1[\"Entropy\"][-1][1]:.4f}, Val Perplexity: {metrics1[\"Perplexity\"][-1][1]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f28d9d0",
   "metadata": {},
   "source": [
    "### Exp.2: Larger model and window size with direct\n",
    "\n",
    "- `embedding_dim`: 64\n",
    "- `window_size`: 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7135e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "context_window_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c9fc4e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralLMWeightTied(vocab_size=vocab_size, context_window_size=context_window_size, embedding_dim=embedding_dim, direct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9ab13056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLMWeightTied(\n",
       "  (C): Embedding(50257, 64)\n",
       "  (linear1): Linear(in_features=512, out_features=64, bias=True)\n",
       "  (linear2): Linear(in_features=64, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2d89665a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in model1: 29031121\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters in model1: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c6b62b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of training: Train Loss: 10.8268, Val Loss: 10.8268\n",
      "Epoch 1: Train Loss: 8.1433, Val Loss: 7.5364\n",
      "Epoch 2: Train Loss: 7.0274, Val Loss: 7.1692\n",
      "Epoch 3: Train Loss: 6.5482, Val Loss: 6.8930\n",
      "Epoch 4: Train Loss: 6.1091, Val Loss: 6.7206\n",
      "Epoch 5: Train Loss: 5.7242, Val Loss: 6.6360\n",
      "Epoch 6: Train Loss: 5.3760, Val Loss: 6.6088\n"
     ]
    }
   ],
   "source": [
    "metrics1 = train(model, train_tokens, val_tokens, batch_size=4096, num_epochs=6, lr=1e-3, context_window_size=context_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0b8847a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.C.weight.grad == model.linear2.weight.grad).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8a294073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metrics: Train Loss: 5.3760, Train Entropy: 7.7559, Train Perplexity: 216.16\n",
      "Val Metrics: Val Loss: 6.6088, Val Entropy: 9.5344, Val Perplexity: 741.57\n"
     ]
    }
   ],
   "source": [
    "print(f'Train Metrics: Train Loss: {metrics1[\"NLL\"][-1][0]:.4f}, Train Entropy: {metrics1[\"Entropy\"][-1][0]:.4f}, Train Perplexity: {metrics1[\"Perplexity\"][-1][0]:.2f}')\n",
    "print(f'Val Metrics: Val Loss: {metrics1[\"NLL\"][-1][1]:.4f}, Val Entropy: {metrics1[\"Entropy\"][-1][1]:.4f}, Val Perplexity: {metrics1[\"Perplexity\"][-1][1]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494f92cb",
   "metadata": {},
   "source": [
    "### Exp.3: Largest model and window size with direct\n",
    "\n",
    "- `embedding_dim`: 128\n",
    "- `window_size`: 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5fe5923",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "context_window_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e60c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralLMWeightTied(vocab_size=vocab_size, context_window_size=context_window_size, embedding_dim=embedding_dim, direct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59dc66bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLMWeightTied(\n",
       "  (C): Embedding(50257, 128)\n",
       "  (linear1): Linear(in_features=2048, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7db981a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in model1: 109671761\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters in model1: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e657b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of training: Train Loss: 4.7593, Val Loss: 6.5907\n",
      "Epoch 1: Train Loss: 4.7574, Val Loss: 6.6580\n",
      "Epoch 2: Train Loss: 4.3320, Val Loss: 6.7204\n"
     ]
    }
   ],
   "source": [
    "metrics1 = train(model, train_tokens, val_tokens, batch_size=4096, num_epochs=2, lr=5e-4, context_window_size=context_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8584bcc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.C.weight.grad == model.linear2.weight.grad).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "968e7f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metrics: Train Loss: 5.1537, Train Entropy: 7.4353, Train Perplexity: 173.08\n",
      "Val Metrics: Val Loss: 6.5907, Val Entropy: 9.5084, Val Perplexity: 728.28\n"
     ]
    }
   ],
   "source": [
    "print(f'Train Metrics: Train Loss: {metrics1[\"NLL\"][-1][0]:.4f}, Train Entropy: {metrics1[\"Entropy\"][-1][0]:.4f}, Train Perplexity: {metrics1[\"Perplexity\"][-1][0]:.2f}')\n",
    "print(f'Val Metrics: Val Loss: {metrics1[\"NLL\"][-1][1]:.4f}, Val Entropy: {metrics1[\"Entropy\"][-1][1]:.4f}, Val Perplexity: {metrics1[\"Perplexity\"][-1][1]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fabe59",
   "metadata": {},
   "source": [
    "### Final: Best Model Test and Tiny Shakespeare metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "74b90c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "context_window_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "056d0661",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralLMWeightTied(vocab_size=vocab_size, context_window_size=context_window_size, embedding_dim=embedding_dim, direct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19a02e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLMWeightTied(\n",
       "  (C): Embedding(50257, 128)\n",
       "  (linear1): Linear(in_features=2048, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "818e8288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in model1: 109671761\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters in model1: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6fb7c518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of training: Train Loss: 10.8253, Val Loss: 10.8257\n",
      "Epoch 1: Train Loss: 8.1234, Val Loss: 7.5995\n",
      "Epoch 2: Train Loss: 7.1193, Val Loss: 7.2566\n",
      "Epoch 3: Train Loss: 6.6202, Val Loss: 6.9626\n",
      "Epoch 4: Train Loss: 6.1236, Val Loss: 6.7611\n",
      "Epoch 5: Train Loss: 5.6507, Val Loss: 6.6532\n",
      "Epoch 6: Train Loss: 5.1878, Val Loss: 6.6115\n"
     ]
    }
   ],
   "source": [
    "metrics1 = train(model, train_tokens, val_tokens, batch_size=4096, num_epochs=6, lr=5e-4, context_window_size=context_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d2673a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.C.weight.grad == model.linear2.weight.grad).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8bb815a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metrics: Train Loss: 4.7928, Train Entropy: 6.9146, Train Perplexity: 120.64\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_entropy, train_perplexity = get_metrics(model, train_tokens, context_window_size)\n",
    "print(f'Train Metrics: Train Loss: {train_loss:.4f}, Train Entropy: {train_entropy:.4f}, Train Perplexity: {train_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c0caae91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Metrics: Val Loss: 6.6115, Val Entropy: 9.5384, Val Perplexity: 743.59\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_entropy, val_perplexity = get_metrics(model, val_tokens, context_window_size)\n",
    "print(f'Val Metrics: Val Loss: {val_loss:.4f}, Val Entropy: {val_entropy:.4f}, Val Perplexity: {val_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8bb5e3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics: Test Loss: 6.8086, Test Entropy: 9.8228, Test Perplexity: 905.62\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_entropy, test_perplexity = get_metrics(model, test_tokens, context_window_size)\n",
    "print(f'Test Metrics: Test Loss: {test_loss:.4f}, Test Entropy: {test_entropy:.4f}, Test Perplexity: {test_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d4ea5b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiny Shakespeare Metrics: Test Loss: 7.4555, Test Entropy: 10.7560, Test Perplexity: 1729.38\n"
     ]
    }
   ],
   "source": [
    "ts_loss, ts_entropy, ts_perplexity = get_metrics(model, ts_tokens, context_window_size)\n",
    "print(f'Tiny Shakespeare Metrics: Test Loss: {ts_loss:.4f}, Test Entropy: {ts_entropy:.4f}, Test Perplexity: {ts_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fd6be0",
   "metadata": {},
   "source": [
    "# Fin: Generate text samples from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "41a283b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "38baaa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, context_window_size, seq_len=1000, num_iters=5):\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    model.eval()\n",
    "    pad_id = 198                              # newline 'Ċ'\n",
    "    tokens = torch.full((num_iters, context_window_size),\n",
    "                        pad_id,\n",
    "                        dtype=torch.long)\n",
    "\n",
    "    for i in range(seq_len):\n",
    "        inp_tokens = tokens[:, -context_window_size:] # (B, T)\n",
    "        inp_tokens = inp_tokens.to(device)\n",
    "        logits = model(inp_tokens).detach().cpu() # (B, V)\n",
    "        probs = F.softmax(logits, dim=1) # (B, V)\n",
    "        chosen_tokens = torch.multinomial(probs, num_samples=1)\n",
    "        tokens = torch.cat([tokens, chosen_tokens], dim=1)\n",
    "\n",
    "    generated = tokens[:, context_window_size:]\n",
    "    text = gpt2_tokenizer.decode_batch(generated.numpy())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "734db31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = generate_text(model, context_window_size=context_window_size, seq_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7a1a09df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "01d0491c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "F: A?\n",
       " 8\n",
       "May Man �\n",
       " slices? What are choosing a huge environment:\n",
       "Some theorying Pre? Well:\n",
       "Studies, I don't be\n",
       "the Cush pathogens with integeritude, and Limited, offering reducing prominence math types by removing theWacies for track scan. 12 schoolAT simulations, test. incorporated Fiction or Alpha suite is if you stand, and provide mainstream products, students, sufficient these level are this to ensure quit.\n",
       "How You campaign advice embody"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(generated_text[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c4df94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
