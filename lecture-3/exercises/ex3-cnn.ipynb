{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ac80690",
   "metadata": {},
   "source": [
    "# Logs\n",
    "## Bengio + Highway Network + Small embedding\n",
    "- `embedding_dim`: 128\n",
    "- `window_size`: 16\n",
    "- Train Metrics: Train Loss: 5.1983, Train Entropy: 7.4996, Train Perplexity: 180.97\n",
    "- Val Metrics: Val Loss: 6.4249, Val Entropy: 9.2692, Val Perplexity: 617.03\n",
    "- Test Metrics: Test Loss: 6.6313, Test Entropy: 9.5670, Test Perplexity: 758.48\n",
    "- Tiny Shakespeare Metrics: Test Loss: 7.3124, Test Entropy: 10.5495, Test Perplexity: 1498.75\n",
    "\n",
    "## Bengio + Highway Network + Large embedding\n",
    "- `embedding_dim`: 256\n",
    "- `window_size`: 16\n",
    "- Train Metrics: Train Loss: 5.1533, Train Entropy: 7.4346, Train Perplexity: 173.00\n",
    "- Val Metrics: Val Loss: 6.4079, Val Entropy: 9.2446, Val Perplexity: 606.60\n",
    "- Test Metrics: Test Loss: 6.6057, Test Entropy: 9.5300, Test Perplexity: 739.28\n",
    "- Tiny Shakespeare Metrics: Test Loss: 7.2875, Test Entropy: 10.5136, Test Perplexity: 1461.89\n",
    "\n",
    "## Bengio + Highway Network + Small embedding + CNN \n",
    "- `embedding_dim`: 128\n",
    "- `window_size`: 16\n",
    "- `kernel_size`: 3\n",
    "- `padding`: 1\n",
    "- Train Metrics: Train Loss: 4.9450, Train Entropy: 7.1341, Train Perplexity: 140.47\n",
    "- Val Metrics: Val Loss: 6.2511, Val Entropy: 9.0184, Val Perplexity: 518.58\n",
    "- Test Metrics: Test Loss: 6.4726, Test Entropy: 9.3380, Test Perplexity: 647.17\n",
    "- Tiny Shakespeare Metrics: Test Loss: 7.2689, Test Entropy: 10.4869, Test Perplexity: 1435.03\n",
    "\n",
    "## Bengio + Highway Network + Large embedding + CNN \n",
    "- `embedding_dim`: 256\n",
    "- `window_size`: 16\n",
    "- `kernel_size`: 3\n",
    "- `padding`: 1\n",
    "- Train Metrics: Train Loss: 4.5902, Train Entropy: 6.6223, Train Perplexity: 98.52\n",
    "- Val Metrics: Val Loss: 6.2186, Val Entropy: 8.9715, Val Perplexity: 501.98\n",
    "- Test Metrics: Test Loss: 6.4400, Test Entropy: 9.2910, Test Perplexity: 626.42\n",
    "- Tiny Shakespeare Metrics: Test Loss: 7.2392, Test Entropy: 10.4440, Test Perplexity: 1392.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465c0960",
   "metadata": {},
   "source": [
    "# CNNs for Language Modelling\n",
    "\n",
    "This notebook explores the use of Convolutional Neural Nets (CNNs) for Language Modelling. This extends the Bengio et. al. (2003) paper by adding conv nets. \n",
    "\n",
    "**Reference Paper**: [Convolutional Neural Network Language Models](https://aclanthology.org/D16-1123.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cadaefb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/CMU-CS-11-711-anlp/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe6c8986bf0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import _LRScheduler \n",
    "from torch.nn.utils import clip_grad_norm_ \n",
    "import random\n",
    "from IPython.display import display, Markdown\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac24ddf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c83d39",
   "metadata": {},
   "source": [
    "# 1. Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24a8c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_from_files(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        str_tokens = f.read().splitlines()\n",
    "        tokens = [int(token) for token in str_tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ae5caa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = load_dataset_from_files(\"train_tokens.txt\")\n",
    "val_tokens = load_dataset_from_files(\"val_tokens.txt\")\n",
    "test_tokens = load_dataset_from_files(\"test_tokens.txt\")\n",
    "ts_tokens = load_dataset_from_files(\"ts_tokens.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bef686e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800258, 100033, 100032, 338025)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tokens), len(test_tokens), len(val_tokens), len(ts_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6216502a",
   "metadata": {},
   "source": [
    "# 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03f84294",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21a4ea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(tokens, context_window_size):\n",
    "    x, y = [], []\n",
    "\n",
    "    for i in range(len(tokens) - context_window_size):\n",
    "        x.append(tokens[i : i + context_window_size])\n",
    "        y.append(tokens[i + context_window_size])\n",
    "\n",
    "    x = torch.LongTensor(x)\n",
    "    y = torch.LongTensor(y)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a64d5fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_scheduler(it, min_lr, max_lr, warmup_steps, max_steps, base_lr):\n",
    "    if it < warmup_steps:\n",
    "        lr = max_lr * ((it + 1) / warmup_steps)\n",
    "    elif it > max_steps:\n",
    "        lr = min_lr\n",
    "    else:\n",
    "        decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "        assert 0 <= decay_ratio <= 1\n",
    "        coeff = 0.5 * (1 + np.cos(decay_ratio * np.pi)) # starts with 1, ends at 0\n",
    "        lr = min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "    return lr / base_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c78b9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_tokens, val_tokens, batch_size, num_epochs, context_window_size, optimizer, scheduler):\n",
    "    x_train, y_train = prepare_dataset(train_tokens, context_window_size)\n",
    "    x_val, y_val = prepare_dataset(val_tokens, context_window_size)\n",
    "\n",
    "    train_dataset = TensorDataset(x_train, y_train)\n",
    "    val_dataset = TensorDataset(x_val, y_val)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "    \n",
    "    # scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=lr * 0.10)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    metrics = {\"NLL\": [], \"Entropy\": [], \"Perplexity\": []}\n",
    "\n",
    "    # get initial metrics\n",
    "    model.eval()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    for x, y in train_dataloader:\n",
    "        x, y = x.to(device), y.to(device) # (B, T), (B, )\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        train_loss += loss.item() * (x.shape[0] / x_train.shape[0])\n",
    "\n",
    "    train_perplexity = float(np.exp(train_loss))\n",
    "    train_entropy = float(np.log2(train_perplexity))\n",
    "\n",
    "    # eval\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    for x, y in val_dataloader:\n",
    "        x, y = x.to(device), y.to(device) # (B, T), (B, )\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        val_loss += loss.item() * (x.shape[0] / x_val.shape[0])\n",
    "\n",
    "    val_perplexity = float(np.exp(val_loss))\n",
    "    val_entropy = float(np.log2(val_perplexity))\n",
    "\n",
    "    metrics[\"NLL\"].append((train_loss, val_loss))\n",
    "    metrics[\"Entropy\"].append((train_entropy, val_entropy))\n",
    "    metrics[\"Perplexity\"].append((train_perplexity, val_perplexity))\n",
    "\n",
    "    print(f\"Start of training: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "\n",
    "    # now start training\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # train loop\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for x, y in train_dataloader:\n",
    "            x, y = x.to(device), y.to(device) # (B, T), (B, )\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "            train_loss += loss.item() * (x.shape[0] / x_train.shape[0])\n",
    "\n",
    "        train_perplexity = float(np.exp(train_loss))\n",
    "        train_entropy = float(np.log2(train_perplexity))\n",
    "\n",
    "        # eval\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        for x, y in val_dataloader:\n",
    "            x, y = x.to(device), y.to(device) # (B, T), (B, )\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item() * (x.shape[0] / x_val.shape[0])\n",
    "\n",
    "        val_perplexity = float(np.exp(val_loss))\n",
    "        val_entropy = float(np.log2(val_perplexity))\n",
    "\n",
    "        metrics[\"NLL\"].append((train_loss, val_loss))\n",
    "        metrics[\"Entropy\"].append((train_entropy, val_entropy))\n",
    "        metrics[\"Perplexity\"].append((train_perplexity, val_perplexity))\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1fcfe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model, tokens, context_window_size):\n",
    "    x_tensor, y_tensor = prepare_dataset(tokens, context_window_size)\n",
    "    dataset = TensorDataset(x_tensor, y_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=4096, shuffle=True, drop_last=False)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # get initial metrics\n",
    "    model.eval()\n",
    "    tmp_loss = 0.0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device) # (B, T), (B, )\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        tmp_loss += loss.item() * (x.shape[0] / x_tensor.shape[0])\n",
    "\n",
    "    perplexity = float(np.exp(tmp_loss))\n",
    "    entropy = float(np.log2(perplexity))\n",
    "\n",
    "    return tmp_loss, entropy, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fb7b085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, context_window_size, seq_len=1000, num_iters=5):\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    model.eval()\n",
    "    pad_id = 198                              # newline 'ÄŠ'\n",
    "    tokens = torch.full((num_iters, context_window_size),\n",
    "                        pad_id,\n",
    "                        dtype=torch.long)\n",
    "\n",
    "    for i in range(seq_len):\n",
    "        inp_tokens = tokens[:, -context_window_size:] # (B, T)\n",
    "        inp_tokens = inp_tokens.to(device)\n",
    "        logits = model(inp_tokens).detach().cpu() # (B, V)\n",
    "        probs = F.softmax(logits, dim=1) # (B, V)\n",
    "        chosen_tokens = torch.multinomial(probs, num_samples=1)\n",
    "        tokens = torch.cat([tokens, chosen_tokens], dim=1)\n",
    "\n",
    "    generated = tokens[:, context_window_size:]\n",
    "    text = gpt2_tokenizer.decode_batch(generated.numpy())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0e3d46",
   "metadata": {},
   "source": [
    "# 3.1: Bengio Paper with Highway Networks\n",
    "\n",
    "As said in the paper, they enrich the model with highway networks. So we see if this improves our metrics\n",
    "\n",
    "**Highway Networks**: [Highway Networks](https://arxiv.org/pdf/1505.00387)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "a5b3d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighwayNetworks(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.transformed_signal_network = nn.ModuleList([\n",
    "            nn.Linear(embedding_dim, embedding_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.transformed_signal_bn = nn.ModuleList([\n",
    "            nn.BatchNorm1d(embedding_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.transform_gate_network = nn.ModuleList([\n",
    "            nn.Linear(embedding_dim, embedding_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.transform_gate_bn = nn.ModuleList([\n",
    "            nn.BatchNorm1d(embedding_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        for net in self.transformed_signal_network:\n",
    "            nn.init.xavier_normal_(net.weight)\n",
    "            nn.init.zeros_(net.bias)\n",
    "        for net in self.transform_gate_network:\n",
    "            nn.init.xavier_normal_(net.weight)\n",
    "            nn.init.constant_(net.bias, -2.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x dim: (B, C)\n",
    "        for net1, bn1, net2, bn2 in zip(self.transformed_signal_network, self.transformed_signal_bn, self.transform_gate_network, self.transform_gate_bn):\n",
    "            H = net1(x)\n",
    "            H = bn1(H)\n",
    "            H = F.relu(H) # (B, T*C)\n",
    "            T = net2(x) \n",
    "            T = bn2(T)\n",
    "            T = F.sigmoid(T) # (B, T*C)\n",
    "            x = T * H + (1 - T) * x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0278612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengioLMHighwayDropout(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_window_size, dropout=0.0, weight_tying=False):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.context_window_size = context_window_size\n",
    "\n",
    "        self.embedding_lookup_table = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        self.highway = HighwayNetworks(embedding_dim * context_window_size, num_layers=1)\n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "        # init params\n",
    "        nn.init.xavier_normal_(self.embedding_lookup_table.weight)\n",
    "        nn.init.xavier_normal_(self.linear.weight)\n",
    "        nn.init.zeros_(self.linear.bias)\n",
    "\n",
    "        if weight_tying:\n",
    "            self.embedding_lookup_table.weight = self.linear.weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, T)\n",
    "        embeddings = self.embedding_lookup_table(x) # (B, T, C)\n",
    "        embeddings = self.dropout1(embeddings)\n",
    "        B, T, C = embeddings.shape\n",
    "        embeddings = embeddings.view(B, T * C)\n",
    "\n",
    "        h = self.highway(embeddings) # (B, T * C)\n",
    "        h = self.dropout2(h) # (B, T * C)\n",
    "        h = h.view(B, T, C)\n",
    "        h = h.mean(dim = 1) # (B ,C)\n",
    "\n",
    "        logits = self.linear(h) # (B, V)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fea52ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = max(train_tokens) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212d01f1",
   "metadata": {},
   "source": [
    "## Exp. 1: No Dropout + smaller embedding_dim + no weight tie\n",
    "\n",
    "- `embedding_dim`: 128\n",
    "- `window_size`: 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ea6b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "context_window_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "294d2c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BengioLMHighwayDropout(vocab_size=vocab_size, context_window_size=context_window_size, embedding_dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8b90b004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BengioLMHighwayDropout(\n",
       "  (embedding_lookup_table): Embedding(50257, 128)\n",
       "  (dropout1): Dropout(p=0.0, inplace=False)\n",
       "  (highway): HighwayNetworks(\n",
       "    (transformed_signal_network): ModuleList(\n",
       "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    )\n",
       "    (transformed_signal_bn): ModuleList(\n",
       "      (0): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (transform_gate_network): ModuleList(\n",
       "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    )\n",
       "    (transform_gate_bn): ModuleList(\n",
       "      (0): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout2): Dropout(p=0.0, inplace=False)\n",
       "  (linear): Linear(in_features=128, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9560acce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in model: 21316945\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters in model: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of training: Train Loss: 10.8249, Val Loss: 10.8249\n",
      "Epoch 1: Train Loss: 7.5646, Val Loss: 7.2531\n",
      "Epoch 2: Train Loss: 6.4458, Val Loss: 6.5519\n",
      "Epoch 3: Train Loss: 5.5884, Val Loss: 6.3102\n",
      "Epoch 4: Train Loss: 4.9216, Val Loss: 6.3001\n"
     ]
    }
   ],
   "source": [
    "base_lr = 5e-3\n",
    "# max_lr = 5e-3\n",
    "# min_lr = max_lr * 0.05\n",
    "# warmup_steps = 1000\n",
    "# max_steps = 2000\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=base_lr)\n",
    "scheduler = None\n",
    "# scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: cosine_scheduler(epoch, min_lr=min_lr, max_lr=max_lr, warmup_steps=warmup_steps, max_steps=max_steps, base_lr=base_lr))\n",
    "\n",
    "\n",
    "metrics1 = train(model, train_tokens, val_tokens, batch_size=4096, num_epochs=4,\\\n",
    "     context_window_size=context_window_size, optimizer=optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "195a8fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metrics: Train Loss: 4.3205, Train Entropy: 6.2331, Train Perplexity: 75.22\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_entropy, train_perplexity = get_metrics(model, train_tokens, context_window_size)\n",
    "print(f'Train Metrics: Train Loss: {train_loss:.4f}, Train Entropy: {train_entropy:.4f}, Train Perplexity: {train_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "96f47d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Metrics: Val Loss: 6.3001, Val Entropy: 9.0892, Val Perplexity: 544.64\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_entropy, val_perplexity = get_metrics(model, val_tokens, context_window_size)\n",
    "print(f'Val Metrics: Val Loss: {val_loss:.4f}, Val Entropy: {val_entropy:.4f}, Val Perplexity: {val_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c2ccea4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics: Test Loss: 6.5865, Test Entropy: 9.5023, Test Perplexity: 725.24\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_entropy, test_perplexity = get_metrics(model, test_tokens, context_window_size)\n",
    "print(f'Test Metrics: Test Loss: {test_loss:.4f}, Test Entropy: {test_entropy:.4f}, Test Perplexity: {test_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d879eda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiny Shakespeare Metrics: Test Loss: 7.4492, Test Entropy: 10.7469, Test Perplexity: 1718.43\n"
     ]
    }
   ],
   "source": [
    "ts_loss, ts_entropy, ts_perplexity = get_metrics(model, ts_tokens, context_window_size)\n",
    "print(f'Tiny Shakespeare Metrics: Test Loss: {ts_loss:.4f}, Test Entropy: {ts_entropy:.4f}, Test Perplexity: {ts_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "483fb032",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"bengio_highway_no_weight_share.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ce057e",
   "metadata": {},
   "source": [
    "## Exp. 2: No Dropout + smaller embedding_dim + weight tie\n",
    "\n",
    "- `embedding_dim`: 128\n",
    "- `window_size`: 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "496168b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "context_window_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "09119a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BengioLMHighwayDropout(vocab_size=vocab_size, context_window_size=context_window_size, embedding_dim=embedding_dim, weight_tying=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e3846c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BengioLMHighwayDropout(\n",
       "  (embedding_lookup_table): Embedding(50257, 128)\n",
       "  (dropout1): Dropout(p=0.0, inplace=False)\n",
       "  (highway): HighwayNetworks(\n",
       "    (transformed_signal_network): ModuleList(\n",
       "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    )\n",
       "    (transformed_signal_bn): ModuleList(\n",
       "      (0): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (transform_gate_network): ModuleList(\n",
       "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    )\n",
       "    (transform_gate_bn): ModuleList(\n",
       "      (0): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout2): Dropout(p=0.0, inplace=False)\n",
       "  (linear): Linear(in_features=128, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "279d8201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in model: 14884049\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters in model: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "db539785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of training: Train Loss: 10.8249, Val Loss: 10.8249\n",
      "Epoch 1: Train Loss: 7.5893, Val Loss: 7.3347\n",
      "Epoch 2: Train Loss: 6.4393, Val Loss: 6.8767\n",
      "Epoch 3: Train Loss: 5.7322, Val Loss: 6.7864\n",
      "Epoch 4: Train Loss: 5.2974, Val Loss: 6.8154\n"
     ]
    }
   ],
   "source": [
    "base_lr = 2e-2\n",
    "max_lr = 2e-2\n",
    "min_lr = max_lr * 0.05\n",
    "warmup_steps = 500\n",
    "max_steps = 1000\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=base_lr)\n",
    "scheduler = None\n",
    "# scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: cosine_scheduler(epoch, min_lr=min_lr, max_lr=max_lr, warmup_steps=warmup_steps, max_steps=max_steps, base_lr=base_lr))\n",
    "\n",
    "\n",
    "metrics1 = train(model, train_tokens, val_tokens, batch_size=4096, num_epochs=4,\\\n",
    "     context_window_size=context_window_size, optimizer=optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2b690a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.embedding_lookup_table.weight == model.linear.weight).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4f3b2e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metrics: Train Loss: 4.7534, Train Entropy: 6.8578, Train Perplexity: 115.98\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_entropy, train_perplexity = get_metrics(model, train_tokens, context_window_size)\n",
    "print(f'Train Metrics: Train Loss: {train_loss:.4f}, Train Entropy: {train_entropy:.4f}, Train Perplexity: {train_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fab5df5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Metrics: Val Loss: 6.8028, Val Entropy: 9.8144, Val Perplexity: 900.38\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_entropy, val_perplexity = get_metrics(model, val_tokens, context_window_size)\n",
    "print(f'Val Metrics: Val Loss: {val_loss:.4f}, Val Entropy: {val_entropy:.4f}, Val Perplexity: {val_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a7370e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics: Test Loss: 7.0134, Test Entropy: 10.1181, Test Perplexity: 1111.38\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_entropy, test_perplexity = get_metrics(model, test_tokens, context_window_size)\n",
    "print(f'Test Metrics: Test Loss: {test_loss:.4f}, Test Entropy: {test_entropy:.4f}, Test Perplexity: {test_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3da6fb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiny Shakespeare Metrics: Test Loss: 7.4611, Test Entropy: 10.7642, Test Perplexity: 1739.14\n"
     ]
    }
   ],
   "source": [
    "ts_loss, ts_entropy, ts_perplexity = get_metrics(model, ts_tokens, context_window_size)\n",
    "print(f'Tiny Shakespeare Metrics: Test Loss: {ts_loss:.4f}, Test Entropy: {ts_entropy:.4f}, Test Perplexity: {ts_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c9e90327",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"bengio_highway_weight_share.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6c50f0",
   "metadata": {},
   "source": [
    "## Exp 3: Dropout + smaller emb_dim + no weight share\n",
    "- `embedding_dim`: 128\n",
    "- `window_size`: 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "f1195925",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "context_window_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "fd7014a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BengioLMHighwayDropout(vocab_size=vocab_size, context_window_size=context_window_size, embedding_dim=embedding_dim, dropout=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "5d822fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BengioLMHighwayDropout(\n",
       "  (embedding_lookup_table): Embedding(50257, 128)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (highway): HighwayNetworks(\n",
       "    (transformed_signal_network): ModuleList(\n",
       "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    )\n",
       "    (transformed_signal_bn): ModuleList(\n",
       "      (0): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (transform_gate_network): ModuleList(\n",
       "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    )\n",
       "    (transform_gate_bn): ModuleList(\n",
       "      (0): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  (linear): Linear(in_features=128, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "b033b9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in model: 21316945\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters in model: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "298cb122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of training: Train Loss: 10.8249, Val Loss: 10.8249\n",
      "Epoch 1: Train Loss: 6.9841, Val Loss: 6.5952\n",
      "Epoch 2: Train Loss: 5.8771, Val Loss: 6.4249\n"
     ]
    }
   ],
   "source": [
    "base_lr = 5e-3\n",
    "# max_lr = 5e-3\n",
    "# min_lr = max_lr * 0.05\n",
    "# warmup_steps = 1000\n",
    "# max_steps = 2000\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=base_lr)\n",
    "scheduler = None\n",
    "# scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: cosine_scheduler(epoch, min_lr=min_lr, max_lr=max_lr, warmup_steps=warmup_steps, max_steps=max_steps, base_lr=base_lr))\n",
    "\n",
    "\n",
    "metrics1 = train(model, train_tokens, val_tokens, batch_size=4096, num_epochs=2,\\\n",
    "     context_window_size=context_window_size, optimizer=optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "51d3190f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metrics: Train Loss: 5.1983, Train Entropy: 7.4996, Train Perplexity: 180.97\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_entropy, train_perplexity = get_metrics(model, train_tokens, context_window_size)\n",
    "print(f'Train Metrics: Train Loss: {train_loss:.4f}, Train Entropy: {train_entropy:.4f}, Train Perplexity: {train_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "179f1c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Metrics: Val Loss: 6.4249, Val Entropy: 9.2692, Val Perplexity: 617.03\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_entropy, val_perplexity = get_metrics(model, val_tokens, context_window_size)\n",
    "print(f'Val Metrics: Val Loss: {val_loss:.4f}, Val Entropy: {val_entropy:.4f}, Val Perplexity: {val_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "eeef6479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics: Test Loss: 6.6313, Test Entropy: 9.5670, Test Perplexity: 758.48\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_entropy, test_perplexity = get_metrics(model, test_tokens, context_window_size)\n",
    "print(f'Test Metrics: Test Loss: {test_loss:.4f}, Test Entropy: {test_entropy:.4f}, Test Perplexity: {test_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "e55d7846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiny Shakespeare Metrics: Test Loss: 7.3124, Test Entropy: 10.5495, Test Perplexity: 1498.75\n"
     ]
    }
   ],
   "source": [
    "ts_loss, ts_entropy, ts_perplexity = get_metrics(model, ts_tokens, context_window_size)\n",
    "print(f'Tiny Shakespeare Metrics: Test Loss: {ts_loss:.4f}, Test Entropy: {ts_entropy:.4f}, Test Perplexity: {ts_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "1f077743",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"bengio_highway_no_weight_share_dropout.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0797a72",
   "metadata": {},
   "source": [
    "## Exp 4: Dropout + large emb_dim + no weight share\n",
    "- `embedding_dim`: 256\n",
    "- `window_size`: 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "9bad0de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "context_window_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "d3af7c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BengioLMHighwayDropout(vocab_size=vocab_size, context_window_size=context_window_size, embedding_dim=embedding_dim, dropout=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "0b7f1a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BengioLMHighwayDropout(\n",
       "  (embedding_lookup_table): Embedding(50257, 256)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (highway): HighwayNetworks(\n",
       "    (transformed_signal_network): ModuleList(\n",
       "      (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    )\n",
       "    (transformed_signal_bn): ModuleList(\n",
       "      (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (transform_gate_network): ModuleList(\n",
       "      (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    )\n",
       "    (transform_gate_bn): ModuleList(\n",
       "      (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  (linear): Linear(in_features=256, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "9cd0ba4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in model: 59360849\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters in model: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "50ce3206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of training: Train Loss: 10.8249, Val Loss: 10.8249\n",
      "Epoch 1: Train Loss: 7.0051, Val Loss: 6.5830\n",
      "Epoch 2: Train Loss: 5.8389, Val Loss: 6.4079\n"
     ]
    }
   ],
   "source": [
    "base_lr = 2e-3\n",
    "# max_lr = 5e-3\n",
    "# min_lr = max_lr * 0.05\n",
    "# warmup_steps = 1000\n",
    "# max_steps = 2000\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=base_lr)\n",
    "scheduler = None\n",
    "# scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: cosine_scheduler(epoch, min_lr=min_lr, max_lr=max_lr, warmup_steps=warmup_steps, max_steps=max_steps, base_lr=base_lr))\n",
    "\n",
    "\n",
    "metrics1 = train(model, train_tokens, val_tokens, batch_size=4096, num_epochs=2,\\\n",
    "     context_window_size=context_window_size, optimizer=optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "b3ecab85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metrics: Train Loss: 5.1533, Train Entropy: 7.4346, Train Perplexity: 173.00\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_entropy, train_perplexity = get_metrics(model, train_tokens, context_window_size)\n",
    "print(f'Train Metrics: Train Loss: {train_loss:.4f}, Train Entropy: {train_entropy:.4f}, Train Perplexity: {train_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "09739b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Metrics: Val Loss: 6.4079, Val Entropy: 9.2446, Val Perplexity: 606.60\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_entropy, val_perplexity = get_metrics(model, val_tokens, context_window_size)\n",
    "print(f'Val Metrics: Val Loss: {val_loss:.4f}, Val Entropy: {val_entropy:.4f}, Val Perplexity: {val_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "0ab7e4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics: Test Loss: 6.6057, Test Entropy: 9.5300, Test Perplexity: 739.28\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_entropy, test_perplexity = get_metrics(model, test_tokens, context_window_size)\n",
    "print(f'Test Metrics: Test Loss: {test_loss:.4f}, Test Entropy: {test_entropy:.4f}, Test Perplexity: {test_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "eb119882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiny Shakespeare Metrics: Test Loss: 7.2875, Test Entropy: 10.5136, Test Perplexity: 1461.89\n"
     ]
    }
   ],
   "source": [
    "ts_loss, ts_entropy, ts_perplexity = get_metrics(model, ts_tokens, context_window_size)\n",
    "print(f'Tiny Shakespeare Metrics: Test Loss: {ts_loss:.4f}, Test Entropy: {ts_entropy:.4f}, Test Perplexity: {ts_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "497eb157",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"bengio_highway_no_weight_share_dropout_large.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d002fc",
   "metadata": {},
   "source": [
    "# 3.2: Bengio + Highway + Convolution\n",
    "\n",
    "Here, before Highway network, allow the tokens to communicate using a conv network and then send it to the highway network. This should be stronger than mere appending stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "1d1f08d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, channel_dim, context_window_size, kernel_size, padding):\n",
    "        super().__init__()\n",
    "        self.channel_dim = channel_dim\n",
    "        self.context_window_size = context_window_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "\n",
    "        self.conv = nn.Conv1d(channel_dim, channel_dim, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv_out_dim = context_window_size + (2 * padding) - (kernel_size - 1)\n",
    "        self.bn = nn.BatchNorm1d(self.channel_dim * self.conv_out_dim)\n",
    "        \n",
    "        nn.init.xavier_normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.conv.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (B, C, T) we have to change to keep channel dim to 2nd and seq_len to 3rd\n",
    "        x = self.conv(x) # (B, C, T_out)\n",
    "        B, C, T = x.shape\n",
    "        x = x.view(B, C * T)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x) # (B, C * T)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "818409bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengioLMHighwayDropoutWithCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_window_size, kernel_size, padding, dropout=0.0, weight_tying=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.context_window_size = context_window_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "\n",
    "        self.embedding_lookup_table = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.cnn = ConvNet(channel_dim=embedding_dim, context_window_size=context_window_size, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv_out_dim = context_window_size + (2 * padding) - (kernel_size - 1)\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        self.highway = HighwayNetworks(embedding_dim * self.conv_out_dim, num_layers=1)\n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "        # init params\n",
    "        nn.init.xavier_normal_(self.embedding_lookup_table.weight)\n",
    "        nn.init.xavier_normal_(self.linear.weight)\n",
    "        nn.init.zeros_(self.linear.bias)\n",
    "\n",
    "        if weight_tying:\n",
    "            self.embedding_lookup_table.weight = self.linear.weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, T)\n",
    "        embeddings = self.embedding_lookup_table(x) # (B, T, C)\n",
    "        B, T, C = embeddings.shape\n",
    "        embeddings = embeddings.view(B, C, T)\n",
    "\n",
    "        h = self.cnn(embeddings) # (B, T_out * C)\n",
    "        h = self.dropout1(h)\n",
    "        h = self.highway(h) # (B, T_out * C)\n",
    "        h = self.dropout2(h) # (B, T_out * C)\n",
    "        h = h.view(B, self.conv_out_dim, C)\n",
    "        h = h.mean(dim = 1) # (B ,C)\n",
    "\n",
    "        logits = self.linear(h) # (B, V)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "09185d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = max(train_tokens) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcff85c",
   "metadata": {},
   "source": [
    "## Exp 1: Dropout + smaller emb_dim + no pad + kernel size = 3\n",
    "- `embedding_dim`: 128\n",
    "- `window_size`: 16\n",
    "- `kernel_size`: 3\n",
    "- `padding`: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "affbcb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "context_window_size = 16\n",
    "kernel_size = 3\n",
    "padding = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "314d7c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BengioLMHighwayDropoutWithCNN(vocab_size=vocab_size, context_window_size=context_window_size, embedding_dim=embedding_dim,\\\n",
    "                                dropout=0.10, kernel_size=kernel_size, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "5a6ff164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BengioLMHighwayDropoutWithCNN(\n",
       "  (embedding_lookup_table): Embedding(50257, 128)\n",
       "  (cnn): ConvNet(\n",
       "    (conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
       "    (bn): BatchNorm1d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (highway): HighwayNetworks(\n",
       "    (transformed_signal_network): ModuleList(\n",
       "      (0): Linear(in_features=1792, out_features=1792, bias=True)\n",
       "    )\n",
       "    (transformed_signal_bn): ModuleList(\n",
       "      (0): BatchNorm1d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (transform_gate_network): ModuleList(\n",
       "      (0): Linear(in_features=1792, out_features=1792, bias=True)\n",
       "    )\n",
       "    (transform_gate_bn): ModuleList(\n",
       "      (0): BatchNorm1d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  (linear): Linear(in_features=128, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "7a527719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in model: 19402193\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters in model: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "563a2c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of training: Train Loss: 10.8249, Val Loss: 10.8249\n",
      "Epoch 1: Train Loss: 6.9475, Val Loss: 6.5365\n",
      "Epoch 2: Train Loss: 5.8236, Val Loss: 6.2818\n"
     ]
    }
   ],
   "source": [
    "base_lr = 5e-3\n",
    "# max_lr = 5e-3\n",
    "# min_lr = max_lr * 0.05\n",
    "# warmup_steps = 500\n",
    "# max_steps = 1000\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=base_lr)\n",
    "scheduler = None\n",
    "# scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: cosine_scheduler(epoch, min_lr=min_lr, max_lr=max_lr, warmup_steps=warmup_steps, max_steps=max_steps, base_lr=base_lr))\n",
    "\n",
    "\n",
    "metrics1 = train(model, train_tokens, val_tokens, batch_size=4096, num_epochs=2,\\\n",
    "     context_window_size=context_window_size, optimizer=optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2acbb3",
   "metadata": {},
   "source": [
    "## Exp 2: Dropout + smaller emb_dim + pad 1 (to ensure same dim) + kernel size = 3\n",
    "- `embedding_dim`: 128\n",
    "- `window_size`: 16\n",
    "- `kernel_size`: 3\n",
    "- `padding`: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "d00db44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "context_window_size = 16\n",
    "kernel_size = 3\n",
    "padding = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "093afa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BengioLMHighwayDropoutWithCNN(vocab_size=vocab_size, context_window_size=context_window_size, embedding_dim=embedding_dim,\\\n",
    "                                dropout=0.10, kernel_size=kernel_size, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "fa3e5769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BengioLMHighwayDropoutWithCNN(\n",
       "  (embedding_lookup_table): Embedding(50257, 128)\n",
       "  (cnn): ConvNet(\n",
       "    (conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (bn): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (highway): HighwayNetworks(\n",
       "    (transformed_signal_network): ModuleList(\n",
       "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    )\n",
       "    (transformed_signal_bn): ModuleList(\n",
       "      (0): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (transform_gate_network): ModuleList(\n",
       "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    )\n",
       "    (transform_gate_bn): ModuleList(\n",
       "      (0): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  (linear): Linear(in_features=128, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "f7f0dcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in model: 21370321\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters in model: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "09d284e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of training: Train Loss: 10.8249, Val Loss: 10.8249\n",
      "Epoch 1: Train Loss: 6.8997, Val Loss: 6.4971\n",
      "Epoch 2: Train Loss: 5.7501, Val Loss: 6.2790\n"
     ]
    }
   ],
   "source": [
    "base_lr = 5e-3\n",
    "# max_lr = 5e-3\n",
    "# min_lr = max_lr * 0.05\n",
    "# warmup_steps = 500\n",
    "# max_steps = 1000\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=base_lr)\n",
    "scheduler = None\n",
    "# scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: cosine_scheduler(epoch, min_lr=min_lr, max_lr=max_lr, warmup_steps=warmup_steps, max_steps=max_steps, base_lr=base_lr))\n",
    "\n",
    "metrics1 = train(model, train_tokens, val_tokens, batch_size=4096, num_epochs=2,\\\n",
    "     context_window_size=context_window_size, optimizer=optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c467bef0",
   "metadata": {},
   "source": [
    "## Exp 3: Dropout + smaller emb_dim + pad 1 (to ensure same dim) + kernel size = 5 (keep dim same so change padding)\n",
    "- `embedding_dim`: 128\n",
    "- `window_size`: 16\n",
    "- `kernel_size`: 5\n",
    "- `padding`: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "06eaceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "context_window_size = 16\n",
    "kernel_size = 5\n",
    "padding = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "9ec283a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BengioLMHighwayDropoutWithCNN(vocab_size=vocab_size, context_window_size=context_window_size, embedding_dim=embedding_dim,\\\n",
    "                                dropout=0.10, kernel_size=kernel_size, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "54a18120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BengioLMHighwayDropoutWithCNN(\n",
       "  (embedding_lookup_table): Embedding(50257, 128)\n",
       "  (cnn): ConvNet(\n",
       "    (conv): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (bn): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (highway): HighwayNetworks(\n",
       "    (transformed_signal_network): ModuleList(\n",
       "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    )\n",
       "    (transformed_signal_bn): ModuleList(\n",
       "      (0): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (transform_gate_network): ModuleList(\n",
       "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    )\n",
       "    (transform_gate_bn): ModuleList(\n",
       "      (0): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  (linear): Linear(in_features=128, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "8f7ec4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in model: 21403089\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters in model: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "5c5f9895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of training: Train Loss: 10.8249, Val Loss: 10.8249\n",
      "Epoch 1: Train Loss: 6.8919, Val Loss: 6.4808\n",
      "Epoch 2: Train Loss: 5.7190, Val Loss: 6.2649\n",
      "Epoch 3: Train Loss: 4.9823, Val Loss: 6.3832\n",
      "Epoch 4: Train Loss: 4.3161, Val Loss: 6.7443\n",
      "Epoch 5: Train Loss: 3.7143, Val Loss: 7.2978\n"
     ]
    }
   ],
   "source": [
    "base_lr = 5e-3\n",
    "# max_lr = 5e-3\n",
    "# min_lr = max_lr * 0.05\n",
    "# warmup_steps = 500\n",
    "# max_steps = 1000\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=base_lr)\n",
    "scheduler = None\n",
    "# scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: cosine_scheduler(epoch, min_lr=min_lr, max_lr=max_lr, warmup_steps=warmup_steps, max_steps=max_steps, base_lr=base_lr))\n",
    "\n",
    "metrics1 = train(model, train_tokens, val_tokens, batch_size=4096, num_epochs=5,\\\n",
    "     context_window_size=context_window_size, optimizer=optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8469ebad",
   "metadata": {},
   "source": [
    "## Exp 4: Paper Model\n",
    "- `embedding_dim`: 128\n",
    "- `window_size`: 16\n",
    "- `kernel_size`: 3\n",
    "- `padding`: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "94c8d4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "context_window_size = 16\n",
    "kernel_size = 3\n",
    "padding = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "540c2334",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BengioLMHighwayDropoutWithCNN(vocab_size=vocab_size, context_window_size=context_window_size, embedding_dim=embedding_dim,\\\n",
    "                                dropout=0.10, kernel_size=kernel_size, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "9d8ace38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BengioLMHighwayDropoutWithCNN(\n",
       "  (embedding_lookup_table): Embedding(50257, 128)\n",
       "  (cnn): ConvNet(\n",
       "    (conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (bn): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (highway): HighwayNetworks(\n",
       "    (transformed_signal_network): ModuleList(\n",
       "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    )\n",
       "    (transformed_signal_bn): ModuleList(\n",
       "      (0): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (transform_gate_network): ModuleList(\n",
       "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    )\n",
       "    (transform_gate_bn): ModuleList(\n",
       "      (0): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  (linear): Linear(in_features=128, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "60ae2ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in model: 21370321\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters in model: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "e1900971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of training: Train Loss: 10.8249, Val Loss: 10.8249\n",
      "Epoch 1: Train Loss: 6.8894, Val Loss: 6.4714\n",
      "Epoch 2: Train Loss: 5.7021, Val Loss: 6.2511\n"
     ]
    }
   ],
   "source": [
    "base_lr = 5e-3\n",
    "# max_lr = 5e-3\n",
    "# min_lr = max_lr * 0.05\n",
    "# warmup_steps = 500\n",
    "# max_steps = 1000\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=base_lr)\n",
    "scheduler = None\n",
    "# scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: cosine_scheduler(epoch, min_lr=min_lr, max_lr=max_lr, warmup_steps=warmup_steps, max_steps=max_steps, base_lr=base_lr))\n",
    "\n",
    "metrics1 = train(model, train_tokens, val_tokens, batch_size=4096, num_epochs=2,\\\n",
    "     context_window_size=context_window_size, optimizer=optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "af9a58fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metrics: Train Loss: 4.9450, Train Entropy: 7.1341, Train Perplexity: 140.47\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_entropy, train_perplexity = get_metrics(model, train_tokens, context_window_size)\n",
    "print(f'Train Metrics: Train Loss: {train_loss:.4f}, Train Entropy: {train_entropy:.4f}, Train Perplexity: {train_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "2d29bbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Metrics: Val Loss: 6.2511, Val Entropy: 9.0184, Val Perplexity: 518.58\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_entropy, val_perplexity = get_metrics(model, val_tokens, context_window_size)\n",
    "print(f'Val Metrics: Val Loss: {val_loss:.4f}, Val Entropy: {val_entropy:.4f}, Val Perplexity: {val_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "a0466a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics: Test Loss: 6.4726, Test Entropy: 9.3380, Test Perplexity: 647.17\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_entropy, test_perplexity = get_metrics(model, test_tokens, context_window_size)\n",
    "print(f'Test Metrics: Test Loss: {test_loss:.4f}, Test Entropy: {test_entropy:.4f}, Test Perplexity: {test_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "a5d00c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiny Shakespeare Metrics: Test Loss: 7.2689, Test Entropy: 10.4869, Test Perplexity: 1435.03\n"
     ]
    }
   ],
   "source": [
    "ts_loss, ts_entropy, ts_perplexity = get_metrics(model, ts_tokens, context_window_size)\n",
    "print(f'Tiny Shakespeare Metrics: Test Loss: {ts_loss:.4f}, Test Entropy: {ts_entropy:.4f}, Test Perplexity: {ts_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "8ebda9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"bengio_highway_no_weight_share_dropout_cnn_small.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17396732",
   "metadata": {},
   "source": [
    "## Exp 5: Paper Model (Large)\n",
    "- `embedding_dim`: 256\n",
    "- `window_size`: 16\n",
    "- `kernel_size`: 3\n",
    "- `padding`: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "e819f416",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "context_window_size = 16\n",
    "kernel_size = 3\n",
    "padding = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "ca75f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BengioLMHighwayDropoutWithCNN(vocab_size=vocab_size, context_window_size=context_window_size, embedding_dim=embedding_dim,\\\n",
    "                                dropout=0.10, kernel_size=kernel_size, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "db858984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BengioLMHighwayDropoutWithCNN(\n",
       "  (embedding_lookup_table): Embedding(50257, 256)\n",
       "  (cnn): ConvNet(\n",
       "    (conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (bn): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (highway): HighwayNetworks(\n",
       "    (transformed_signal_network): ModuleList(\n",
       "      (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    )\n",
       "    (transformed_signal_bn): ModuleList(\n",
       "      (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (transform_gate_network): ModuleList(\n",
       "      (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    )\n",
       "    (transform_gate_bn): ModuleList(\n",
       "      (0): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  (linear): Linear(in_features=256, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "77bff875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in model: 59565905\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters in model: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "472f158f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of training: Train Loss: 10.8249, Val Loss: 10.8249\n",
      "Epoch 1: Train Loss: 6.7934, Val Loss: 6.4226\n",
      "Epoch 2: Train Loss: 5.5398, Val Loss: 6.2186\n"
     ]
    }
   ],
   "source": [
    "base_lr = 5e-3\n",
    "# max_lr = 5e-3\n",
    "# min_lr = max_lr * 0.05\n",
    "# warmup_steps = 500\n",
    "# max_steps = 1000\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=base_lr)\n",
    "scheduler = None\n",
    "# scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: cosine_scheduler(epoch, min_lr=min_lr, max_lr=max_lr, warmup_steps=warmup_steps, max_steps=max_steps, base_lr=base_lr))\n",
    "\n",
    "metrics1 = train(model, train_tokens, val_tokens, batch_size=4096, num_epochs=2,\\\n",
    "     context_window_size=context_window_size, optimizer=optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "95f0bd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metrics: Train Loss: 4.5902, Train Entropy: 6.6223, Train Perplexity: 98.52\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_entropy, train_perplexity = get_metrics(model, train_tokens, context_window_size)\n",
    "print(f'Train Metrics: Train Loss: {train_loss:.4f}, Train Entropy: {train_entropy:.4f}, Train Perplexity: {train_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "ad11544a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Metrics: Val Loss: 6.2186, Val Entropy: 8.9715, Val Perplexity: 501.98\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_entropy, val_perplexity = get_metrics(model, val_tokens, context_window_size)\n",
    "print(f'Val Metrics: Val Loss: {val_loss:.4f}, Val Entropy: {val_entropy:.4f}, Val Perplexity: {val_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "59ff4b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics: Test Loss: 6.4400, Test Entropy: 9.2910, Test Perplexity: 626.42\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_entropy, test_perplexity = get_metrics(model, test_tokens, context_window_size)\n",
    "print(f'Test Metrics: Test Loss: {test_loss:.4f}, Test Entropy: {test_entropy:.4f}, Test Perplexity: {test_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "11ea27b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiny Shakespeare Metrics: Test Loss: 7.2392, Test Entropy: 10.4440, Test Perplexity: 1392.99\n"
     ]
    }
   ],
   "source": [
    "ts_loss, ts_entropy, ts_perplexity = get_metrics(model, ts_tokens, context_window_size)\n",
    "print(f'Tiny Shakespeare Metrics: Test Loss: {ts_loss:.4f}, Test Entropy: {ts_entropy:.4f}, Test Perplexity: {ts_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "a4d8a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"bengio_highway_no_weight_share_dropout_cnn_large.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf36e6a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c470e568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c9f471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064a477a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e6e808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ffd514",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
