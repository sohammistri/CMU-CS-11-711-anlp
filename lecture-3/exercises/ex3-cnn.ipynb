{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465c0960",
   "metadata": {},
   "source": [
    "# CNNs for Language Modelling\n",
    "\n",
    "This notebook explores the use of Convolutional Neural Nets (CNNs) for Language Modelling. This extends the Bengio et. al. (2003) paper by adding conv nets. \n",
    "\n",
    "**Reference Paper**: [Convolutional Neural Network Language Models](https://aclanthology.org/D16-1123.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cadaefb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/CMU-CS-11-711-anlp/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f79782829b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import _LRScheduler \n",
    "from torch.nn.utils import clip_grad_norm_ \n",
    "import random\n",
    "from IPython.display import display, Markdown\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac24ddf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c83d39",
   "metadata": {},
   "source": [
    "# 1. Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24a8c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_from_files(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        str_tokens = f.read().splitlines()\n",
    "        tokens = [int(token) for token in str_tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ae5caa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = load_dataset_from_files(\"train_tokens.txt\")\n",
    "val_tokens = load_dataset_from_files(\"val_tokens.txt\")\n",
    "test_tokens = load_dataset_from_files(\"test_tokens.txt\")\n",
    "ts_tokens = load_dataset_from_files(\"ts_tokens.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bef686e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800258, 100033, 100032, 338025)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tokens), len(test_tokens), len(val_tokens), len(ts_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6216502a",
   "metadata": {},
   "source": [
    "# 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03f84294",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21a4ea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(tokens, context_window_size):\n",
    "    x, y = [], []\n",
    "\n",
    "    for i in range(len(tokens) - context_window_size):\n",
    "        x.append(tokens[i : i + context_window_size])\n",
    "        y.append(tokens[i + context_window_size])\n",
    "\n",
    "    x = torch.LongTensor(x)\n",
    "    y = torch.LongTensor(y)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3a575ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model, tokens, context_window_size):\n",
    "    x_tensor, y_tensor = prepare_dataset(tokens, context_window_size)\n",
    "    dataset = TensorDataset(x_tensor, y_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=4096, shuffle=True, drop_last=False)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # get initial metrics\n",
    "    model.eval()\n",
    "    tmp_loss = 0.0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device) # (B, T), (B, )\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        tmp_loss += loss.item() * (x.shape[0] / x_tensor.shape[0])\n",
    "\n",
    "    perplexity = float(np.exp(tmp_loss))\n",
    "    entropy = float(np.log2(perplexity))\n",
    "\n",
    "    return tmp_loss, entropy, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a64d5fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_scheduler(it, min_lr, max_lr, warmup_steps, max_steps, base_lr):\n",
    "    if it < warmup_steps:\n",
    "        lr = max_lr * ((it + 1) / warmup_steps)\n",
    "    elif it > max_steps:\n",
    "        lr = min_lr\n",
    "    else:\n",
    "        decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "        assert 0 <= decay_ratio <= 1\n",
    "        coeff = 0.5 * (1 + np.cos(decay_ratio * np.pi)) # starts with 1, ends at 0\n",
    "        lr = min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "    return lr / base_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c78b9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_tokens, val_tokens, batch_size, num_epochs, lr, context_window_size, base_lr, max_lr, min_lr, warmup_steps, max_steps):\n",
    "    x_train, y_train = prepare_dataset(train_tokens, context_window_size)\n",
    "    x_val, y_val = prepare_dataset(val_tokens, context_window_size)\n",
    "\n",
    "    train_dataset = TensorDataset(x_train, y_train)\n",
    "    val_dataset = TensorDataset(x_val, y_val)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: cosine_scheduler(epoch, min_lr=min_lr, max_lr=max_lr, warmup_steps=warmup_steps, max_steps=max_steps, base_lr=base_lr))\n",
    "    # scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=lr * 0.10)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    metrics = {\"NLL\": [], \"Entropy\": [], \"Perplexity\": []}\n",
    "\n",
    "    # get initial metrics\n",
    "    model.eval()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    for x, y in train_dataloader:\n",
    "        x, y = x.to(device), y.to(device) # (B, T), (B, )\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        train_loss += loss.item() * (x.shape[0] / x_train.shape[0])\n",
    "\n",
    "    train_perplexity = float(np.exp(train_loss))\n",
    "    train_entropy = float(np.log2(train_perplexity))\n",
    "\n",
    "    # eval\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    for x, y in val_dataloader:\n",
    "        x, y = x.to(device), y.to(device) # (B, T), (B, )\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        val_loss += loss.item() * (x.shape[0] / x_val.shape[0])\n",
    "\n",
    "    val_perplexity = float(np.exp(val_loss))\n",
    "    val_entropy = float(np.log2(val_perplexity))\n",
    "\n",
    "    metrics[\"NLL\"].append((train_loss, val_loss))\n",
    "    metrics[\"Entropy\"].append((train_entropy, val_entropy))\n",
    "    metrics[\"Perplexity\"].append((train_perplexity, val_perplexity))\n",
    "\n",
    "    print(f\"Start of training: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "\n",
    "    # now start training\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # train loop\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for x, y in train_dataloader:\n",
    "            x, y = x.to(device), y.to(device) # (B, T), (B, )\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_loss += loss.item() * (x.shape[0] / x_train.shape[0])\n",
    "\n",
    "        train_perplexity = float(np.exp(train_loss))\n",
    "        train_entropy = float(np.log2(train_perplexity))\n",
    "\n",
    "        # eval\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        for x, y in val_dataloader:\n",
    "            x, y = x.to(device), y.to(device) # (B, T), (B, )\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item() * (x.shape[0] / x_val.shape[0])\n",
    "\n",
    "        val_perplexity = float(np.exp(val_loss))\n",
    "        val_entropy = float(np.log2(val_perplexity))\n",
    "\n",
    "        metrics[\"NLL\"].append((train_loss, val_loss))\n",
    "        metrics[\"Entropy\"].append((train_entropy, val_entropy))\n",
    "        metrics[\"Perplexity\"].append((train_perplexity, val_perplexity))\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1fcfe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model, tokens, context_window_size):\n",
    "    x_tensor, y_tensor = prepare_dataset(tokens, context_window_size)\n",
    "    dataset = TensorDataset(x_tensor, y_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=4096, shuffle=True, drop_last=False)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # get initial metrics\n",
    "    model.eval()\n",
    "    tmp_loss = 0.0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device) # (B, T), (B, )\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        tmp_loss += loss.item() * (x.shape[0] / x_tensor.shape[0])\n",
    "\n",
    "    perplexity = float(np.exp(tmp_loss))\n",
    "    entropy = float(np.log2(perplexity))\n",
    "\n",
    "    return tmp_loss, entropy, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fb7b085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, context_window_size, seq_len=1000, num_iters=5):\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    model.eval()\n",
    "    pad_id = 198                              # newline 'ÄŠ'\n",
    "    tokens = torch.full((num_iters, context_window_size),\n",
    "                        pad_id,\n",
    "                        dtype=torch.long)\n",
    "\n",
    "    for i in range(seq_len):\n",
    "        inp_tokens = tokens[:, -context_window_size:] # (B, T)\n",
    "        inp_tokens = inp_tokens.to(device)\n",
    "        logits = model(inp_tokens).detach().cpu() # (B, V)\n",
    "        probs = F.softmax(logits, dim=1) # (B, V)\n",
    "        chosen_tokens = torch.multinomial(probs, num_samples=1)\n",
    "        tokens = torch.cat([tokens, chosen_tokens], dim=1)\n",
    "\n",
    "    generated = tokens[:, context_window_size:]\n",
    "    text = gpt2_tokenizer.decode_batch(generated.numpy())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0e3d46",
   "metadata": {},
   "source": [
    "# 3.1: Bengio Paper with Highway Networks\n",
    "\n",
    "As said in the paper, they enrich the model with highway networks. So we see if this improves our metrics\n",
    "\n",
    "**Highway Networks**: [Highway Networks](https://arxiv.org/pdf/1505.00387)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5b3d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighwayNetworks(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.transformed_signal_network = nn.ModuleList([\n",
    "            nn.Linear(embedding_dim, embedding_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.transformed_signal_bn = nn.ModuleList([\n",
    "            nn.BatchNorm1d(embedding_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.transform_gate_network = nn.ModuleList([\n",
    "            nn.Linear(embedding_dim, embedding_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.transform_gate_bn = nn.ModuleList([\n",
    "            nn.BatchNorm1d(embedding_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        for net in self.transformed_signal_network:\n",
    "            nn.init.xavier_normal_(net.weight)\n",
    "            nn.init.zeros_(net.bias)\n",
    "        for net in self.transform_gate_network:\n",
    "            nn.init.xavier_normal_(net.weight)\n",
    "            nn.init.zeros_(net.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x dim: (B, C)\n",
    "        for net1, bn1, net2, bn2 in zip(self.transformed_signal_network, self.transformed_signal_bn, self.transform_gate_network, self.transform_gate_bn):\n",
    "            H = net1(x)\n",
    "            H = bn1(x)\n",
    "            H = F.relu(H) # (B, T*C)\n",
    "            T = net2(x) \n",
    "            T = bn2(x)\n",
    "            T = F.sigmoid(T) # (B, T*C)\n",
    "            x = T * H + (1 - T) * x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0278612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengioLMHighwayDropout(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_window_size, dropout=0.0, weight_tying=False):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.context_window_size = context_window_size\n",
    "\n",
    "        self.embedding_lookup_table = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        self.linear1 = nn.Linear(embedding_dim * context_window_size, embedding_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(embedding_dim)\n",
    "        self.highway = HighwayNetworks(embedding_dim, num_layers=1)\n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "        self.linear2 = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "        # init params\n",
    "        nn.init.xavier_normal_(self.embedding_lookup_table.weight)\n",
    "        nn.init.xavier_normal_(self.linear1.weight)\n",
    "        nn.init.zeros_(self.linear1.bias)\n",
    "        nn.init.xavier_normal_(self.linear2.weight)\n",
    "        nn.init.zeros_(self.linear2.bias)\n",
    "\n",
    "        if weight_tying:\n",
    "            self.embedding_lookup_table.weight = self.linear2.weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, T)\n",
    "        embeddings = self.embedding_lookup_table(x) # (B, T, C)\n",
    "        embeddings = self.dropout1(embeddings)\n",
    "        B, T, C = embeddings.shape\n",
    "        embeddings = embeddings.view(B, T * C)\n",
    "\n",
    "        h = self.linear1(embeddings) # (B, C)\n",
    "        h = self.bn1(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.highway(h) # (B, C)\n",
    "        h = self.dropout2(h) # (B, C)\n",
    "\n",
    "        logits = self.linear2(h) # (B, V)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fea52ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = max(train_tokens) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212d01f1",
   "metadata": {},
   "source": [
    "## Exp. 1: No Dropout + smaller embedding_dim + no weight tie\n",
    "\n",
    "- `embedding_dim`: 128\n",
    "- `window_size`: 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9ea6b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "context_window_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "294d2c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BengioLMHighwayDropout(vocab_size=vocab_size, context_window_size=context_window_size, embedding_dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8b90b004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BengioLMHighwayDropout(\n",
       "  (embedding_lookup_table): Embedding(50257, 128)\n",
       "  (dropout1): Dropout(p=0.0, inplace=False)\n",
       "  (linear1): Linear(in_features=2048, out_features=128, bias=True)\n",
       "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (highway): HighwayNetworks(\n",
       "    (transformed_signal_network): ModuleList(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (transformed_signal_bn): ModuleList(\n",
       "      (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (transform_gate_network): ModuleList(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (transform_gate_bn): ModuleList(\n",
       "      (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout2): Dropout(p=0.0, inplace=False)\n",
       "  (linear2): Linear(in_features=128, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9560acce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in model: 13212113\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters in model: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7e837449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "976.87744140625"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(train_tokens) / 4096) * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of training: Train Loss: 10.8249, Val Loss: 10.8249\n",
      "Epoch 1: Train Loss: 8.0440, Val Loss: 6.8911\n",
      "Epoch 2: Train Loss: 6.3218, Val Loss: 6.6711\n",
      "Epoch 3: Train Loss: 5.9790, Val Loss: 6.6695\n",
      "Epoch 4: Train Loss: 5.9473, Val Loss: 6.6697\n",
      "Epoch 5: Train Loss: 5.9330, Val Loss: 6.6699\n"
     ]
    }
   ],
   "source": [
    "base_lr = 1e-3\n",
    "max_lr = 1e-3\n",
    "min_lr = max_lr * 0.01\n",
    "warmup_steps = 50\n",
    "max_steps = 500\n",
    "\n",
    "metrics1 = train(model, train_tokens, val_tokens, batch_size=4096, num_epochs=5, lr=base_lr, context_window_size=context_window_size,\\\n",
    "                  base_lr=base_lr, max_lr=max_lr, min_lr=min_lr, warmup_steps=warmup_steps, max_steps=max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "195a8fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metrics: Train Loss: 5.9225, Train Entropy: 8.5443, Train Perplexity: 373.33\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_entropy, train_perplexity = get_metrics(model, train_tokens, context_window_size)\n",
    "print(f'Train Metrics: Train Loss: {train_loss:.4f}, Train Entropy: {train_entropy:.4f}, Train Perplexity: {train_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "96f47d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Metrics: Val Loss: 6.6699, Val Entropy: 9.6226, Val Perplexity: 788.29\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_entropy, val_perplexity = get_metrics(model, val_tokens, context_window_size)\n",
    "print(f'Val Metrics: Val Loss: {val_loss:.4f}, Val Entropy: {val_entropy:.4f}, Val Perplexity: {val_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c2ccea4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics: Test Loss: 6.8268, Test Entropy: 9.8490, Test Perplexity: 922.22\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_entropy, test_perplexity = get_metrics(model, test_tokens, context_window_size)\n",
    "print(f'Test Metrics: Test Loss: {test_loss:.4f}, Test Entropy: {test_entropy:.4f}, Test Perplexity: {test_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d879eda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiny Shakespeare Metrics: Test Loss: 7.2134, Test Entropy: 10.4067, Test Perplexity: 1357.47\n"
     ]
    }
   ],
   "source": [
    "ts_loss, ts_entropy, ts_perplexity = get_metrics(model, ts_tokens, context_window_size)\n",
    "print(f'Tiny Shakespeare Metrics: Test Loss: {ts_loss:.4f}, Test Entropy: {ts_entropy:.4f}, Test Perplexity: {ts_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ce057e",
   "metadata": {},
   "source": [
    "## Exp. 2: No Dropout + smaller embedding_dim + weight tie\n",
    "\n",
    "- `embedding_dim`: 128\n",
    "- `window_size`: 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "496168b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "context_window_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "09119a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BengioLMHighwayDropout(vocab_size=vocab_size, context_window_size=context_window_size, embedding_dim=embedding_dim, weight_tying=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e3846c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BengioLMHighwayDropout(\n",
       "  (embedding_lookup_table): Embedding(50257, 128)\n",
       "  (dropout1): Dropout(p=0.0, inplace=False)\n",
       "  (linear1): Linear(in_features=2048, out_features=128, bias=True)\n",
       "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (highway): HighwayNetworks(\n",
       "    (transformed_signal_network): ModuleList(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (transformed_signal_bn): ModuleList(\n",
       "      (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (transform_gate_network): ModuleList(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (transform_gate_bn): ModuleList(\n",
       "      (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout2): Dropout(p=0.0, inplace=False)\n",
       "  (linear2): Linear(in_features=128, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "279d8201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in model: 6779217\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters in model: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "db539785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of training: Train Loss: 10.8249, Val Loss: 10.8249\n",
      "Epoch 1: Train Loss: 8.1074, Val Loss: 6.8830\n",
      "Epoch 2: Train Loss: 6.2607, Val Loss: 6.5313\n",
      "Epoch 3: Train Loss: 5.6692, Val Loss: 6.4632\n"
     ]
    }
   ],
   "source": [
    "base_lr = 2e-3\n",
    "max_lr = 2e-3\n",
    "min_lr = max_lr * 0.005\n",
    "warmup_steps = 100\n",
    "max_steps = 1000\n",
    "\n",
    "metrics1 = train(model, train_tokens, val_tokens, batch_size=4096, num_epochs=3, lr=base_lr, context_window_size=context_window_size,\\\n",
    "                  base_lr=base_lr, max_lr=max_lr, min_lr=min_lr, warmup_steps=warmup_steps, max_steps=max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2b690a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.embedding_lookup_table.weight == model.linear2.weight).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4f3b2e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metrics: Train Loss: 5.2638, Train Entropy: 7.5941, Train Perplexity: 193.22\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_entropy, train_perplexity = get_metrics(model, train_tokens, context_window_size)\n",
    "print(f'Train Metrics: Train Loss: {train_loss:.4f}, Train Entropy: {train_entropy:.4f}, Train Perplexity: {train_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "fab5df5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Metrics: Val Loss: 6.4632, Val Entropy: 9.3244, Val Perplexity: 641.10\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_entropy, val_perplexity = get_metrics(model, val_tokens, context_window_size)\n",
    "print(f'Val Metrics: Val Loss: {val_loss:.4f}, Val Entropy: {val_entropy:.4f}, Val Perplexity: {val_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a7370e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics: Test Loss: 6.6800, Test Entropy: 9.6372, Test Perplexity: 796.30\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_entropy, test_perplexity = get_metrics(model, test_tokens, context_window_size)\n",
    "print(f'Test Metrics: Test Loss: {test_loss:.4f}, Test Entropy: {test_entropy:.4f}, Test Perplexity: {test_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3da6fb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiny Shakespeare Metrics: Test Loss: 7.2875, Test Entropy: 10.5136, Test Perplexity: 1461.91\n"
     ]
    }
   ],
   "source": [
    "ts_loss, ts_entropy, ts_perplexity = get_metrics(model, ts_tokens, context_window_size)\n",
    "print(f'Tiny Shakespeare Metrics: Test Loss: {ts_loss:.4f}, Test Entropy: {ts_entropy:.4f}, Test Perplexity: {ts_perplexity:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e90327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6c50f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0797a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
