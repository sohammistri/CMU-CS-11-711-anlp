{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465c0960",
   "metadata": {},
   "source": [
    "# CNNs for Language Modelling\n",
    "\n",
    "This notebook explores the use of Convolutional Neural Nets (CNNs) for Language Modelling. This extends the Bengio et. al. (2003) paper by adding conv nets. \n",
    "\n",
    "**Reference Paper**: [Convolutional Neural Network Language Models](https://aclanthology.org/D16-1123.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cadaefb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe36ac7a8f0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import _LRScheduler \n",
    "from torch.nn.utils import clip_grad_norm_ \n",
    "import random\n",
    "from IPython.display import display, Markdown\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac24ddf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c83d39",
   "metadata": {},
   "source": [
    "# 1. Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24a8c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_from_files(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        str_tokens = f.read().splitlines()\n",
    "        tokens = [int(token) for token in str_tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ae5caa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = load_dataset_from_files(\"train_tokens.txt\")\n",
    "val_tokens = load_dataset_from_files(\"val_tokens.txt\")\n",
    "test_tokens = load_dataset_from_files(\"test_tokens.txt\")\n",
    "ts_tokens = load_dataset_from_files(\"ts_tokens.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bef686e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800258, 100033, 100032, 338025)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tokens), len(test_tokens), len(val_tokens), len(ts_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6216502a",
   "metadata": {},
   "source": [
    "# 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03f84294",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21a4ea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(tokens, context_window_size):\n",
    "    x, y = [], []\n",
    "\n",
    "    for i in range(len(tokens) - context_window_size):\n",
    "        x.append(tokens[i : i + context_window_size])\n",
    "        y.append(tokens[i + context_window_size])\n",
    "\n",
    "    x = torch.LongTensor(x)\n",
    "    y = torch.LongTensor(y)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3a575ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model, tokens, context_window_size):\n",
    "    x_tensor, y_tensor = prepare_dataset(tokens, context_window_size)\n",
    "    dataset = TensorDataset(x_tensor, y_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=4096, shuffle=True, drop_last=False)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # get initial metrics\n",
    "    model.eval()\n",
    "    tmp_loss = 0.0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device) # (B, T), (B, )\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        tmp_loss += loss.item() * (x.shape[0] / x_tensor.shape[0])\n",
    "\n",
    "    perplexity = float(np.exp(tmp_loss))\n",
    "    entropy = float(np.log2(perplexity))\n",
    "\n",
    "    return tmp_loss, entropy, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a64d5fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_scheduler(it, min_lr, max_lr, warmup_steps, max_steps, base_lr):\n",
    "    if it < warmup_steps:\n",
    "        lr = max_lr * ((it + 1) / warmup_steps)\n",
    "    elif it > max_steps:\n",
    "        lr = min_lr\n",
    "    else:\n",
    "        decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "        assert 0 <= decay_ratio <= 1\n",
    "        coeff = 0.5 * (1 + np.cos(decay_ratio * np.pi)) # starts with 1, ends at 0\n",
    "        lr = min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "    return lr / base_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7c78b9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_tokens, val_tokens, batch_size, num_epochs, lr, context_window_size, base_lr, max_lr, min_lr, warmup_steps, max_steps):\n",
    "    x_train, y_train = prepare_dataset(train_tokens, context_window_size)\n",
    "    x_val, y_val = prepare_dataset(val_tokens, context_window_size)\n",
    "\n",
    "    train_dataset = TensorDataset(x_train, y_train)\n",
    "    val_dataset = TensorDataset(x_val, y_val)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: cosine_scheduler(epoch, min_lr=min_lr, max_lr=max_lr, warmup_steps=warmup_steps, max_steps=max_steps, base_lr=base_lr))\n",
    "    # scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=lr * 0.10)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    metrics = {\"NLL\": [], \"Entropy\": [], \"Perplexity\": []}\n",
    "\n",
    "    # get initial metrics\n",
    "    model.eval()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    for x, y in train_dataloader:\n",
    "        x, y = x.to(device), y.to(device) # (B, T), (B, )\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        train_loss += loss.item() * (x.shape[0] / x_train.shape[0])\n",
    "\n",
    "    train_perplexity = float(np.exp(train_loss))\n",
    "    train_entropy = float(np.log2(train_perplexity))\n",
    "\n",
    "    # eval\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    for x, y in val_dataloader:\n",
    "        x, y = x.to(device), y.to(device) # (B, T), (B, )\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        val_loss += loss.item() * (x.shape[0] / x_val.shape[0])\n",
    "\n",
    "    val_perplexity = float(np.exp(val_loss))\n",
    "    val_entropy = float(np.log2(val_perplexity))\n",
    "\n",
    "    metrics[\"NLL\"].append((train_loss, val_loss))\n",
    "    metrics[\"Entropy\"].append((train_entropy, val_entropy))\n",
    "    metrics[\"Perplexity\"].append((train_perplexity, val_perplexity))\n",
    "\n",
    "    print(f\"Start of training: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "\n",
    "    # now start training\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # train loop\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for x, y in train_dataloader:\n",
    "            x, y = x.to(device), y.to(device) # (B, T), (B, )\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_loss += loss.item() * (x.shape[0] / x_train.shape[0])\n",
    "\n",
    "        train_perplexity = float(np.exp(train_loss))\n",
    "        train_entropy = float(np.log2(train_perplexity))\n",
    "\n",
    "        # eval\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        for x, y in val_dataloader:\n",
    "            x, y = x.to(device), y.to(device) # (B, T), (B, )\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item() * (x.shape[0] / x_val.shape[0])\n",
    "\n",
    "        val_perplexity = float(np.exp(val_loss))\n",
    "        val_entropy = float(np.log2(val_perplexity))\n",
    "\n",
    "        metrics[\"NLL\"].append((train_loss, val_loss))\n",
    "        metrics[\"Entropy\"].append((train_entropy, val_entropy))\n",
    "        metrics[\"Perplexity\"].append((train_perplexity, val_perplexity))\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b1fcfe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model, tokens, context_window_size):\n",
    "    x_tensor, y_tensor = prepare_dataset(tokens, context_window_size)\n",
    "    dataset = TensorDataset(x_tensor, y_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=4096, shuffle=True, drop_last=False)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # get initial metrics\n",
    "    model.eval()\n",
    "    tmp_loss = 0.0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device) # (B, T), (B, )\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        tmp_loss += loss.item() * (x.shape[0] / x_tensor.shape[0])\n",
    "\n",
    "    perplexity = float(np.exp(tmp_loss))\n",
    "    entropy = float(np.log2(perplexity))\n",
    "\n",
    "    return tmp_loss, entropy, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8fb7b085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, context_window_size, seq_len=1000, num_iters=5):\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    model.eval()\n",
    "    pad_id = 198                              # newline 'Ċ'\n",
    "    tokens = torch.full((num_iters, context_window_size),\n",
    "                        pad_id,\n",
    "                        dtype=torch.long)\n",
    "\n",
    "    for i in range(seq_len):\n",
    "        inp_tokens = tokens[:, -context_window_size:] # (B, T)\n",
    "        inp_tokens = inp_tokens.to(device)\n",
    "        logits = model(inp_tokens).detach().cpu() # (B, V)\n",
    "        probs = F.softmax(logits, dim=1) # (B, V)\n",
    "        chosen_tokens = torch.multinomial(probs, num_samples=1)\n",
    "        tokens = torch.cat([tokens, chosen_tokens], dim=1)\n",
    "\n",
    "    generated = tokens[:, context_window_size:]\n",
    "    text = gpt2_tokenizer.decode_batch(generated.numpy())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0e3d46",
   "metadata": {},
   "source": [
    "# 3.1: Bengio Paper with Highway Networks\n",
    "\n",
    "As said in the paper, they enrich the model with highway networks. So we see if this improves our metrics\n",
    "\n",
    "**Highway Networks**: [Highway Networks](https://arxiv.org/pdf/1505.00387)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b3d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighwayNetworks(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.transformed_signal_network = nn.ModuleList([\n",
    "            nn.Linear(embedding_dim, embedding_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.transform_gate_network = nn.ModuleList([\n",
    "            nn.Linear(embedding_dim, embedding_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        for net in self.transformed_signal_network:\n",
    "            nn.init.xavier_normal_(net.weight)\n",
    "            nn.init.zeros_(net.bias)\n",
    "        for net in self.transform_gate_network:\n",
    "            nn.init.xavier_normal_(net.weight)\n",
    "            nn.init.zeros_(net.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x dim: (B, C)\n",
    "        for net1, net2 in zip(self.transformed_signal_network, self.transform_gate_network):\n",
    "            H = net1(x)\n",
    "            H = \n",
    "            H = F.relu(H) # (B, T*C)\n",
    "            T = net2(x) \n",
    "            T = F.sigmoid(T) # (B, T*C)\n",
    "            x = T * H + (1 - T) * x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0278612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengioLMHighwayDropout(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_window_size, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.context_window_size = context_window_size\n",
    "\n",
    "        self.embedding_lookup_table = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.dropout1 = nn.Dropout(p=dropout)\n",
    "        self.linear1 = nn.Linear(embedding_dim * context_window_size, embedding_dim)\n",
    "        self.highway = HighwayNetworks(embedding_dim, num_layers=1)\n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "        self.linear2 = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "        # init params\n",
    "        nn.init.xavier_normal_(self.embedding_lookup_table.weight)\n",
    "        nn.init.xavier_normal_(self.linear1.weight)\n",
    "        nn.init.zeros_(self.linear1.bias)\n",
    "        nn.init.xavier_normal_(self.linear2.weight)\n",
    "        nn.init.zeros_(self.linear2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, T)\n",
    "        embeddings = self.embedding_lookup_table(x) # (B, T, C)\n",
    "        embeddings = self.dropout1(embeddings)\n",
    "        B, T, C = embeddings.shape\n",
    "        embeddings = embeddings.view(B, T * C)\n",
    "\n",
    "        h = self.linear1(embeddings) # (B, C)\n",
    "        h = F.relu(h)\n",
    "        h = self.highway(h) # (B, C)\n",
    "        h = self.dropout2(h) # (B, C)\n",
    "\n",
    "        logits = self.linear2(h) # (B, V)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9fea52ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = max(train_tokens) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212d01f1",
   "metadata": {},
   "source": [
    "## Exp. 1: No Dropout + smaller embedding_dim\n",
    "\n",
    "- `embedding_dim`: 128\n",
    "- `window_size`: 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9ea6b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "context_window_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "294d2c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BengioLMHighwayDropout(vocab_size=vocab_size, context_window_size=context_window_size, embedding_dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8b90b004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BengioLMHighwayDropout(\n",
       "  (embedding_lookup_table): Embedding(50257, 128)\n",
       "  (dropout1): Dropout(p=0.0, inplace=False)\n",
       "  (linear1): Linear(in_features=2048, out_features=128, bias=True)\n",
       "  (highway): HighwayNetworks(\n",
       "    (transformed_signal_network): ModuleList(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (transform_gate_network): ModuleList(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout2): Dropout(p=0.0, inplace=False)\n",
       "  (linear2): Linear(in_features=128, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9560acce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in model: 13211345\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters in model: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7e837449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1953.7548828125"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(train_tokens) / 4096) * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of training: Train Loss: 10.8249, Val Loss: 10.8249\n",
      "Epoch 1: Train Loss: 8.7199, Val Loss: 7.6741\n",
      "Epoch 2: Train Loss: 7.4034, Val Loss: 7.5457\n",
      "Epoch 3: Train Loss: 7.2861, Val Loss: 7.5031\n",
      "Epoch 4: Train Loss: 7.1611, Val Loss: 7.4311\n",
      "Epoch 5: Train Loss: 7.0751, Val Loss: 7.4074\n",
      "Epoch 6: Train Loss: 7.0377, Val Loss: 7.3925\n",
      "Epoch 7: Train Loss: 7.0063, Val Loss: 7.3746\n",
      "Epoch 8: Train Loss: 6.9715, Val Loss: 7.3556\n",
      "Epoch 9: Train Loss: 6.9345, Val Loss: 7.3395\n",
      "Epoch 10: Train Loss: 6.8970, Val Loss: 7.3263\n"
     ]
    }
   ],
   "source": [
    "base_lr = 5e-4\n",
    "max_lr = 5e-4\n",
    "min_lr = max_lr * 0.10\n",
    "warmup_steps = 100\n",
    "max_steps = 1000\n",
    "\n",
    "metrics1 = train(model, train_tokens, val_tokens, batch_size=4096, num_epochs=10, lr=base_lr, context_window_size=context_window_size,\\\n",
    "                  base_lr=base_lr, max_lr=max_lr, min_lr=min_lr, warmup_steps=warmup_steps, max_steps=max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aa4fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    " Train Loss: 7.0286, Val Loss: 7.4028\n",
    "Train Loss: 6.8970, Val Loss: 7.3263"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
